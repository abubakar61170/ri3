{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c2145d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Authorships (Seats) : 28,010\n",
      "Unique Author IDs (People): 17,208\n",
      "Missing Author IDs        : 153\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "INPUT_FILE = Path(os.getenv(\"ICCS_FINAL\"))\n",
    "\n",
    "def audit_unique_authors():\n",
    "    unique_ids = set()\n",
    "    total_authorships = 0\n",
    "    missing_id_count = 0\n",
    "    \n",
    "    with open(INPUT_FILE, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                rec = json.loads(line)\n",
    "                # authorships contains the link between work and person\n",
    "                authorships = rec.get('openalex_work', {}).get('authorships', [])\n",
    "                \n",
    "                for auth in authorships:\n",
    "                    total_authorships += 1\n",
    "                    a_id_raw = auth.get('author', {}).get('id')\n",
    "                    \n",
    "                    if a_id_raw:\n",
    "                        # NORMALIZATION: \"https://openalex.org/A123\" -> \"A123\"\n",
    "                        clean_id = str(a_id_raw).split('/')[-1].strip().upper()\n",
    "                        unique_ids.add(clean_id)\n",
    "                    else:\n",
    "                        missing_id_count += 1\n",
    "            except: continue\n",
    "\n",
    "    print(f\"Total Authorships (Seats) : {total_authorships:,}\")\n",
    "    print(f\"Unique Author IDs (People): {len(unique_ids):,}\")\n",
    "    print(f\"Missing Author IDs        : {missing_id_count}\")\n",
    "    \n",
    "    return unique_ids\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    gold_ids = audit_unique_authors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefb6888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status: 17208 Total | 10 Fetched | 17198 Remaining\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batch Fetching: 100%|██████████| 344/344 [22:24<00:00,  3.91s/batch]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Logger configuration\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='author_profile.log'\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "INPUT_FILE = Path(os.getenv(\"ICCS_FINAL\", \"iccs_final.jsonl\")) # Fallback added for testing\n",
    "OUTPUT_FILE = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\iccs_author_profiles.jsonl\")\n",
    "EMAIL = os.getenv(\"OPENALEX_EMAIL\", \"nehalsonu4@gmail.com\")\n",
    "BATCH_SIZE = 50  # OpenAlex max filter size\n",
    "\n",
    "# Session management\n",
    "session = requests.Session()\n",
    "retries = Retry(\n",
    "    total=5,\n",
    "    backoff_factor=0.5, # Reduced backoff for faster recovery\n",
    "    status_forcelist=[429, 500, 502, 503, 504]\n",
    ")\n",
    "adapter = HTTPAdapter(pool_connections=20, pool_maxsize=20, max_retries=retries)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "def normalize_id(raw_id: str) -> str:\n",
    "    \"\"\"Standardizes author identifiers.\"\"\"\n",
    "    if not raw_id:\n",
    "        return None\n",
    "    return str(raw_id).split('/')[-1].strip().upper()\n",
    "\n",
    "def get_unique_author_ids(path: Path) -> list:\n",
    "    \"\"\"Extracts unique author IDs from the source file.\"\"\"\n",
    "    authors = set()\n",
    "    if not path.exists():\n",
    "        logging.error(f\"Input file not found: {path}\")\n",
    "        return []\n",
    "        \n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    rec = json.loads(line)\n",
    "                    # Handle both direct list and openalex_work wrapper\n",
    "                    authorships = rec.get('openalex_work', {}).get('authorships', []) or rec.get('authorships', [])\n",
    "                    for auth in authorships:\n",
    "                        a_id = normalize_id(auth.get('author', {}).get('id'))\n",
    "                        if a_id:\n",
    "                            authors.add(a_id)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error reading {path}: {e}\")\n",
    "    return list(authors)\n",
    "\n",
    "def get_already_fetched_ids(path: Path) -> set:\n",
    "    \"\"\"Retrieves IDs from existing output.\"\"\"\n",
    "    fetched = set()\n",
    "    if path.exists():\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    a_id = normalize_id(data.get('id'))\n",
    "                    if a_id:\n",
    "                        fetched.add(a_id)\n",
    "                except:\n",
    "                    continue\n",
    "    return fetched\n",
    "\n",
    "def fetch_author_batch(batch_ids: list) -> list:\n",
    "    \"\"\"Retrieves up to 50 author profiles in a single request.\"\"\"\n",
    "    if not batch_ids:\n",
    "        return []\n",
    "        \n",
    "    # Join IDs with pipe for OR filter\n",
    "    ids_filter = \"|\".join(batch_ids)\n",
    "    url = \"https://api.openalex.org/authors\"\n",
    "    \n",
    "    params = {\n",
    "        \"filter\": f\"openalex:{ids_filter}\",\n",
    "        \"per-page\": 50, # Critical: must match batch size\n",
    "        \"mailto\": EMAIL\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = session.get(url, params=params, timeout=30)\n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            return data.get('results', [])\n",
    "        elif response.status_code == 429:\n",
    "            logging.warning(\"Rate limit hit. Sleeping...\")\n",
    "            time.sleep(2)\n",
    "            return fetch_author_batch(batch_ids) # Simple recursive retry\n",
    "        else:\n",
    "            logging.error(f\"Batch failed {response.status_code}: {response.text}\")\n",
    "            return []\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Request failed for batch: {e}\")\n",
    "        return []\n",
    "\n",
    "def chunked_iterable(iterable, size):\n",
    "    \"\"\"Yield successive n-sized chunks from iterable.\"\"\"\n",
    "    for i in range(0, len(iterable), size):\n",
    "        yield iterable[i:i + size]\n",
    "\n",
    "def main():\n",
    "    all_ids = get_unique_author_ids(INPUT_FILE)\n",
    "    fetched_ids = get_already_fetched_ids(OUTPUT_FILE)\n",
    "    \n",
    "    remaining_ids = list(set(all_ids) - fetched_ids)\n",
    "    \n",
    "    print(f\"Status: {len(all_ids)} Total | {len(fetched_ids)} Fetched | {len(remaining_ids)} Remaining\")\n",
    "    \n",
    "    if not remaining_ids:\n",
    "        print(\"Data is synchronized. No fetching required.\")\n",
    "        return\n",
    "\n",
    "    OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Create batches\n",
    "    batches = list(chunked_iterable(remaining_ids, BATCH_SIZE))\n",
    "    \n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f_out:\n",
    "        with ThreadPoolExecutor(max_workers=5) as executor:\n",
    "            # We map over batches, not individual IDs\n",
    "            futures = {executor.submit(fetch_author_batch, batch): batch for batch in batches}\n",
    "            \n",
    "            for future in tqdm(as_completed(futures), total=len(batches), desc=\"Batch Fetching\", unit=\"batch\"):\n",
    "                results = future.result()\n",
    "                if results:\n",
    "                    for profile in results:\n",
    "                        f_out.write(json.dumps(profile, ensure_ascii=False) + \"\\n\")\n",
    "                        \n",
    "    print(\"Fetching complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "571a393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading files...\n",
      "Total: 17208 | Fetched: 16057 | Remaining: 1151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Final Cleanup: 100%|██████████| 1151/1151 [16:07<00:00,  1.19it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    filename='author_cleanup.log' # New log file for this run\n",
    ")\n",
    "\n",
    "# Files\n",
    "INPUT_FILE = Path(os.getenv(\"ICCS_FINAL\", \"iccs_final.jsonl\"))\n",
    "OUTPUT_FILE = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\iccs_author_profiles.jsonl\")\n",
    "EMAIL = os.getenv(\"OPENALEX_EMAIL\", \"nehalsonu4@gmail.com\")\n",
    "\n",
    "# Robust Session\n",
    "session = requests.Session()\n",
    "retries = Retry(total=3, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "adapter = HTTPAdapter(max_retries=retries)\n",
    "session.mount(\"https://\", adapter)\n",
    "\n",
    "def normalize_id(raw_id):\n",
    "    if not raw_id: return None\n",
    "    return str(raw_id).split('/')[-1].strip().upper()\n",
    "\n",
    "def get_ids(path, is_output=False):\n",
    "    ids = set()\n",
    "    if not path.exists(): return ids\n",
    "    try:\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                try:\n",
    "                    data = json.loads(line)\n",
    "                    # Input file has structure inside 'authorships', Output is flat\n",
    "                    if is_output:\n",
    "                        val = data.get('id')\n",
    "                    else:\n",
    "                        # Scan all possible authors in the input line\n",
    "                        authorships = data.get('openalex_work', {}).get('authorships', []) or data.get('authorships', [])\n",
    "                        for auth in authorships:\n",
    "                            a_id = normalize_id(auth.get('author', {}).get('id'))\n",
    "                            if a_id: ids.add(a_id)\n",
    "                        continue # Skip the direct val check for input\n",
    "                    \n",
    "                    if val: ids.add(normalize_id(val))\n",
    "                except: continue\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {path}: {e}\")\n",
    "    return ids\n",
    "\n",
    "def main():\n",
    "    print(\"Reading files...\")\n",
    "    all_ids = get_ids(INPUT_FILE, is_output=False)\n",
    "    fetched_ids = get_ids(OUTPUT_FILE, is_output=True)\n",
    "    \n",
    "    # Calculate strictly what is missing\n",
    "    remaining = list(all_ids - fetched_ids)\n",
    "    print(f\"Total: {len(all_ids)} | Fetched: {len(fetched_ids)} | Remaining: {len(remaining)}\")\n",
    "    \n",
    "    if not remaining:\n",
    "        print(\"Done!\")\n",
    "        return\n",
    "\n",
    "    # Open in Append mode\n",
    "    with open(OUTPUT_FILE, 'a', encoding='utf-8') as f:\n",
    "        for auth_id in tqdm(remaining, desc=\"Final Cleanup\"):\n",
    "            url = f\"https://api.openalex.org/authors/{auth_id}?mailto={EMAIL}\"\n",
    "            \n",
    "            try:\n",
    "                # 1. Polite delay\n",
    "                time.sleep(0.6) \n",
    "                \n",
    "                # 2. Request\n",
    "                resp = session.get(url, timeout=10)\n",
    "                \n",
    "                if resp.status_code == 200:\n",
    "                    data = resp.json()\n",
    "                    f.write(json.dumps(data) + \"\\n\")\n",
    "                    f.flush() # Save immediately\n",
    "                elif resp.status_code == 404:\n",
    "                    logging.warning(f\"Author {auth_id} not found (404)\")\n",
    "                elif resp.status_code == 429:\n",
    "                    print(\"Hit 429. Sleeping 10s...\")\n",
    "                    time.sleep(10)\n",
    "                else:\n",
    "                    logging.error(f\"Failed {auth_id}: {resp.status_code}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error fetching {auth_id}: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680aa333",
   "metadata": {},
   "source": [
    "Fetching all papers of these aurthors using openalex "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "088aa693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTHOR WORK DISTRIBUTION (OVERALL)\n",
      "=============================================\n",
      "Total Authors Analyzed    : 17,207\n",
      "Authors with >=1 Work      : 17,207\n",
      "Total Papers (All Time)   : 2,077,990\n",
      "Percentage of Prolific    : 100.0%\n",
      "\n",
      "ERA-SPECIFIC AUDIT (2001-2025) [cite: 17, 67]\n",
      "=============================================\n",
      "Estimated Era Papers      : 1,885,872\n",
      "Reduction from Total      : 9.2%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Path to your library of 17,207 authors\n",
    "AUTHOR_PROFILES_PATH = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\iccs_author_profiles.jsonl\")\n",
    "\n",
    "def run_combined_audit():\n",
    "    total_authors = 0\n",
    "    total_expected_works = 0\n",
    "    more_than_one = 0\n",
    "    era_works_total = 0\n",
    "    \n",
    "    if not AUTHOR_PROFILES_PATH.exists():\n",
    "        print(f\"Error: File not found at {AUTHOR_PROFILES_PATH}\")\n",
    "        return\n",
    "\n",
    "    with open(AUTHOR_PROFILES_PATH, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                total_authors += 1\n",
    "                \n",
    "                # Part 1: General Distribution\n",
    "                count = data.get('works_count', 0)\n",
    "                total_expected_works += count\n",
    "                if count >= 1:\n",
    "                    more_than_one += 1\n",
    "                \n",
    "                # Part 2: Era-Specific (2001-2025) Audit [cite: 6, 67]\n",
    "                yearly_stats = data.get('counts_by_year', [])\n",
    "                for entry in yearly_stats:\n",
    "                    year = entry.get('year')\n",
    "                    if 2001 <= year <= 2025:\n",
    "                        era_works_total += entry.get('works_count', 0)\n",
    "                        \n",
    "            except Exception as e:\n",
    "                continue\n",
    "\n",
    "    print(f\"AUTHOR WORK DISTRIBUTION (OVERALL)\")\n",
    "    print(\"=\"*45)\n",
    "    print(f\"Total Authors Analyzed    : {total_authors:,}\")\n",
    "    print(f\"Authors with >=1 Work      : {more_than_one:,}\")\n",
    "    print(f\"Total Papers (All Time)   : {total_expected_works:,}\")\n",
    "    print(f\"Percentage of Prolific    : {(more_than_one/total_authors)*100:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nERA-SPECIFIC AUDIT (2001-2025) [cite: 17, 67]\")\n",
    "    print(\"=\"*45)\n",
    "    print(f\"Estimated Era Papers      : {era_works_total:,}\")\n",
    "    reduction = ((total_expected_works - era_works_total) / total_expected_works) * 100 if total_expected_works > 0 else 0\n",
    "    print(f\"Reduction from Total      : {reduction:.1f}%\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_combined_audit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d403b67",
   "metadata": {},
   "source": [
    "### Fetch ICCS Author Works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb0853e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: duckdb in c:\\anaconda3\\envs\\ri3_embed\\lib\\site-packages (1.4.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Authors: 17,207 | Batch size: 500 | Batches: 35\n",
      "[batch 00000] sanity ok: page results=191\n",
      "[batch 00000] authors 0-499 | rows 61,653 | 1.1 min\n",
      "[batch 00001] sanity ok: page results=8\n",
      "[batch 00001] authors 500-999 | rows 65,985 | 1.7 min\n",
      "[batch 00002] sanity ok: page results=12\n",
      "[batch 00002] authors 1,000-1,499 | rows 49,846 | 0.9 min\n",
      "[batch 00003] sanity ok: page results=6\n",
      "[batch 00003] authors 1,500-1,999 | rows 58,159 | 1.1 min\n",
      "[batch 00004] sanity ok: page results=10\n",
      "[batch 00004] authors 2,000-2,499 | rows 55,091 | 1.0 min\n",
      "[batch 00005] sanity ok: page results=20\n",
      "[batch 00005] authors 2,500-2,999 | rows 54,305 | 1.0 min\n",
      "[batch 00006] sanity ok: page results=8\n",
      "[batch 00006] authors 3,000-3,499 | rows 60,915 | 1.2 min\n",
      "[batch 00007] sanity ok: page results=7\n",
      "[batch 00007] authors 3,500-3,999 | rows 54,849 | 1.1 min\n",
      "[batch 00008] sanity ok: page results=7\n",
      "[batch 00008] authors 4,000-4,499 | rows 57,003 | 1.1 min\n",
      "[batch 00009] sanity ok: page results=9\n",
      "[batch 00009] authors 4,500-4,999 | rows 55,871 | 1.1 min\n",
      "[batch 00010] sanity ok: page results=5\n",
      "[batch 00010] authors 5,000-5,499 | rows 86,485 | 3.3 min\n",
      "[batch 00011] sanity ok: page results=9\n",
      "[batch 00011] authors 5,500-5,999 | rows 58,200 | 1.2 min\n",
      "[batch 00012] sanity ok: page results=5\n",
      "[batch 00012] authors 6,000-6,499 | rows 50,520 | 1.0 min\n",
      "[batch 00013] sanity ok: page results=11\n",
      "[batch 00013] authors 6,500-6,999 | rows 66,652 | 2.3 min\n",
      "[batch 00014] sanity ok: page results=6\n",
      "[batch 00014] authors 7,000-7,499 | rows 55,456 | 1.1 min\n",
      "[batch 00015] sanity ok: page results=8\n",
      "[batch 00015] authors 7,500-7,999 | rows 53,164 | 1.1 min\n",
      "[batch 00016] sanity ok: page results=14\n",
      "[batch 00016] authors 8,000-8,499 | rows 52,840 | 1.1 min\n",
      "[batch 00017] sanity ok: page results=7\n",
      "[batch 00017] authors 8,500-8,999 | rows 54,579 | 2.9 min\n",
      "[batch 00018] sanity ok: page results=3\n",
      "[batch 00018] authors 9,000-9,499 | rows 66,125 | 1.6 min\n",
      "[batch 00019] sanity ok: page results=12\n",
      "[batch 00019] authors 9,500-9,999 | rows 53,943 | 1.0 min\n",
      "[batch 00020] sanity ok: page results=2\n",
      "[batch 00020] authors 10,000-10,499 | rows 55,458 | 1.1 min\n",
      "[batch 00021] sanity ok: page results=1\n",
      "[batch 00021] authors 10,500-10,999 | rows 56,023 | 1.0 min\n",
      "[batch 00022] sanity ok: page results=17\n",
      "[batch 00022] authors 11,000-11,499 | rows 61,497 | 1.2 min\n",
      "[batch 00023] sanity ok: page results=4\n",
      "[batch 00023] authors 11,500-11,999 | rows 50,346 | 0.9 min\n",
      "[batch 00024] sanity ok: page results=13\n",
      "[batch 00024] authors 12,000-12,499 | rows 62,574 | 1.5 min\n",
      "[batch 00025] sanity ok: page results=11\n",
      "[batch 00025] authors 12,500-12,999 | rows 69,694 | 1.4 min\n",
      "[batch 00026] sanity ok: page results=3\n",
      "[batch 00026] authors 13,000-13,499 | rows 62,189 | 1.1 min\n",
      "[batch 00027] sanity ok: page results=5\n",
      "[batch 00027] authors 13,500-13,999 | rows 63,687 | 1.1 min\n",
      "[batch 00028] sanity ok: page results=10\n",
      "[batch 00028] authors 14,000-14,499 | rows 58,693 | 1.1 min\n",
      "[batch 00029] sanity ok: page results=6\n",
      "[batch 00029] authors 14,500-14,999 | rows 61,762 | 1.2 min\n",
      "[batch 00030] sanity ok: page results=6\n",
      "[batch 00030] authors 15,000-15,499 | rows 51,695 | 1.0 min\n",
      "[batch 00031] sanity ok: page results=4\n",
      "[batch 00031] authors 15,500-15,999 | rows 56,649 | 1.0 min\n",
      "[batch 00032] sanity ok: page results=6\n",
      "[batch 00032] authors 16,000-16,499 | rows 61,517 | 1.5 min\n",
      "[batch 00033] sanity ok: page results=2\n",
      "[batch 00033] authors 16,500-16,999 | rows 54,382 | 0.9 min\n",
      "[batch 00034] sanity ok: page results=3\n",
      "[batch 00034] authors 17,000-17,206 | rows 26,504 | 0.5 min\n",
      "Done. Total rows written (new): 2,024,311. Total time: 44.3 min\n",
      "Parquet parts in: D:\\ITMO Big Data & ML School\\semester 3\\RI3\\Data\\openalex_works_full_parquet\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import math\n",
    "import random\n",
    "from pathlib import Path\n",
    "from time import perf_counter\n",
    "from urllib.parse import quote\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import aiohttp\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "load_dotenv()\n",
    "\n",
    "INPUT_FILE = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\Data\\processed\\pro_oa_iccs\\iccs_author_profiles.jsonl\")\n",
    "OUT_DIR = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\data\\openalex_works_full_parquet\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "EMAIL = os.getenv(\"OA_EMAIL\")\n",
    "API_KEY = os.getenv(\"OA_API_KEY\")\n",
    "\n",
    "\n",
    "BATCH_SIZE = 500\n",
    "RESUME = True\n",
    "\n",
    "FETCH_WORKERS = 15\n",
    "CONN_LIMIT_PER_HOST = 15\n",
    "MAX_RETRIES = 8\n",
    "\n",
    "PER_PAGE = 200\n",
    "Q_OUT_MAX = FETCH_WORKERS * 4\n",
    "\n",
    "# CHECKS\n",
    "if not API_KEY or API_KEY.strip() in {\"\", \"PASTE_YOUR_REAL_OPENALEX_API_KEY_HERE\"}:\n",
    "    raise ValueError(\"API_KEY is empty/placeholder. Paste OpenAlex api_key.\")\n",
    "API_KEY = API_KEY.strip()\n",
    "EMAIL = EMAIL.strip()\n",
    "\n",
    "try:\n",
    "    import orjson\n",
    "except ImportError:\n",
    "    orjson = None\n",
    "\n",
    "def loads_line(line: str):\n",
    "    if orjson:\n",
    "        return orjson.loads(line)\n",
    "    import json\n",
    "    return json.loads(line)\n",
    "\n",
    "def dumps_json(obj) -> str:\n",
    "    # Store exact API object as JSON string in Parquet\n",
    "    if orjson:\n",
    "        return orjson.dumps(obj).decode(\"utf-8\")\n",
    "    import json\n",
    "    return json.dumps(obj, ensure_ascii=False)\n",
    "\n",
    "\n",
    "# PARQUET SCHEMA\n",
    "# Keep the full work payload in one column as JSON text.\n",
    "# This preserves the API structure exactly.\n",
    "PARQUET_SCHEMA = pa.schema([\n",
    "    (\"author_id\", pa.string()),\n",
    "    (\"work_id\", pa.string()),\n",
    "    (\"work_json\", pa.large_string()),\n",
    "])\n",
    "\n",
    "def to_rows(author_id: str, works: list[dict]) -> list[dict]:\n",
    "    rows = []\n",
    "    for w in works:\n",
    "        rows.append({\n",
    "            \"author_id\": author_id,\n",
    "            \"work_id\": w.get(\"id\"),\n",
    "            \"work_json\": dumps_json(w),\n",
    "        })\n",
    "    return rows\n",
    "\n",
    "# URL HELPERS\n",
    "def add_param(url: str, key: str, value: str) -> str:\n",
    "    v = quote(str(value), safe=\"\")\n",
    "    sep = \"&\" if \"?\" in url else \"?\"\n",
    "    return f\"{url}{sep}{key}={v}\"\n",
    "\n",
    "def build_works_url(works_api_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Build a URL that returns FULL Work objects (no select=...).\n",
    "    Use cursor paging for robustness. :contentReference[oaicite:2]{index=2}\n",
    "    \"\"\"\n",
    "    url = works_api_url\n",
    "    url = add_param(url, \"mailto\", EMAIL)\n",
    "    url = add_param(url, \"api_key\", API_KEY)\n",
    "    url = add_param(url, \"per-page\", str(PER_PAGE))\n",
    "    # cursor paging start\n",
    "    url = add_param(url, \"cursor\", \"*\")\n",
    "    return url\n",
    "\n",
    "# HTTP FETCH WITH RETRIES (hard fail on auth)\n",
    "async def fetch_json(session: aiohttp.ClientSession, url: str) -> dict | None:\n",
    "    for attempt in range(MAX_RETRIES):\n",
    "        try:\n",
    "            async with session.get(url) as r:\n",
    "                if r.status == 200:\n",
    "                    return await r.json()\n",
    "\n",
    "                if r.status in (401, 403):\n",
    "                    txt = await r.text()\n",
    "                    raise RuntimeError(f\"AUTH ERROR {r.status}: {txt[:800]}\")\n",
    "\n",
    "                if r.status == 429:\n",
    "                    delay = min(60, (2 ** attempt)) + random.random()\n",
    "                    await asyncio.sleep(delay)\n",
    "                    continue\n",
    "\n",
    "                if 500 <= r.status < 600:\n",
    "                    delay = min(30, (2 ** attempt)) + random.random()\n",
    "                    await asyncio.sleep(delay)\n",
    "                    continue\n",
    "\n",
    "                return None\n",
    "\n",
    "        except (aiohttp.ClientError, asyncio.TimeoutError):\n",
    "            delay = min(30, (2 ** attempt)) + random.random()\n",
    "            await asyncio.sleep(delay)\n",
    "\n",
    "    return None\n",
    "\n",
    "# FETCH ALL WORKS FOR ONE AUTHOR (cursor paging)\n",
    "async def fetch_author_works_full(session: aiohttp.ClientSession, author: dict) -> dict | None:\n",
    "    author_id = author.get(\"id\")\n",
    "    works_api_url = author.get(\"works_api_url\")\n",
    "    if not works_api_url:\n",
    "        return None\n",
    "\n",
    "    base = build_works_url(works_api_url)\n",
    "    works = []\n",
    "\n",
    "    cursor = \"*\"\n",
    "    while True:\n",
    "        url = add_param(base.split(\"&cursor=\")[0], \"cursor\", cursor) if \"&cursor=\" in base else add_param(base, \"cursor\", cursor)\n",
    "        data = await fetch_json(session, url)\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        results = data.get(\"results\") or []\n",
    "        if not results:\n",
    "            break\n",
    "\n",
    "        works.extend(results)\n",
    "\n",
    "        # Cursor paging: next_cursor in meta :contentReference[oaicite:3]{index=3}\n",
    "        meta = data.get(\"meta\") or {}\n",
    "        next_cursor = meta.get(\"next_cursor\")\n",
    "        if not next_cursor or next_cursor == cursor:\n",
    "            break\n",
    "        cursor = next_cursor\n",
    "\n",
    "    if not works:\n",
    "        return None\n",
    "\n",
    "    return {\"author_id\": author_id, \"works\": works}\n",
    "\n",
    "# WORKERS + WRITER\n",
    "async def fetch_worker(session: aiohttp.ClientSession, q_in: asyncio.Queue, q_out: asyncio.Queue):\n",
    "    while True:\n",
    "        author = await q_in.get()\n",
    "        if author is None:\n",
    "            q_in.task_done()\n",
    "            break\n",
    "        try:\n",
    "            result = await fetch_author_works_full(session, author)\n",
    "            if result and result.get(\"works\"):\n",
    "                await q_out.put(result)\n",
    "        finally:\n",
    "            q_in.task_done()\n",
    "\n",
    "async def parquet_writer_worker(q_out: asyncio.Queue, out_path: Path) -> int:\n",
    "    writer = None\n",
    "    rows_written = 0\n",
    "    try:\n",
    "        while True:\n",
    "            item = await q_out.get()\n",
    "            if item is None:\n",
    "                q_out.task_done()\n",
    "                break\n",
    "\n",
    "            aid = item[\"author_id\"]\n",
    "            rows = to_rows(aid, item[\"works\"])\n",
    "            if rows:\n",
    "                table = pa.Table.from_pylist(rows, schema=PARQUET_SCHEMA)\n",
    "                if writer is None:\n",
    "                    writer = pq.ParquetWriter(\n",
    "                        str(out_path),\n",
    "                        PARQUET_SCHEMA,\n",
    "                        compression=\"zstd\",\n",
    "                        use_dictionary=True,\n",
    "                    )\n",
    "                writer.write_table(table)\n",
    "                rows_written += len(rows)\n",
    "\n",
    "            q_out.task_done()\n",
    "    finally:\n",
    "        if writer is not None:\n",
    "            writer.close()\n",
    "    return rows_written\n",
    "\n",
    "\n",
    "# RUN ONE BATCH (atomic write: .tmp then rename)\n",
    "async def run_batch(batch_idx: int, authors_batch: list[dict]) -> int:\n",
    "    out_path = OUT_DIR / f\"part-{batch_idx:05d}.parquet\"\n",
    "    tmp_path = OUT_DIR / f\"part-{batch_idx:05d}.parquet.tmp\"\n",
    "\n",
    "    if RESUME and out_path.exists():\n",
    "        print(f\"[batch {batch_idx:05d}] exists -> skipping\")\n",
    "        return 0\n",
    "\n",
    "    # If a previous run crashed mid-batch, remove tmp\n",
    "    if tmp_path.exists():\n",
    "        tmp_path.unlink()\n",
    "\n",
    "    timeout = aiohttp.ClientTimeout(total=120, connect=10, sock_connect=10, sock_read=120)\n",
    "    connector = aiohttp.TCPConnector(limit_per_host=CONN_LIMIT_PER_HOST, limit=0, ttl_dns_cache=300)\n",
    "    headers = {\n",
    "        \"Accept\": \"application/json\",\n",
    "        \"User-Agent\": f\"openalex-full-works-fetch (mailto:{EMAIL})\",\n",
    "    }\n",
    "\n",
    "    q_in = asyncio.Queue()  # unbounded => producer never blocks\n",
    "    q_out = asyncio.Queue(maxsize=Q_OUT_MAX)\n",
    "\n",
    "    async with aiohttp.ClientSession(timeout=timeout, connector=connector, headers=headers) as session:\n",
    "        # sanity: fail immediately if auth wrong\n",
    "        test = await fetch_json(session, add_param(build_works_url(authors_batch[0][\"works_api_url\"]), \"cursor\", \"*\"))\n",
    "        print(f\"[batch {batch_idx:05d}] sanity ok: page results={len((test or {}).get('results') or [])}\")\n",
    "\n",
    "        writer_task = asyncio.create_task(parquet_writer_worker(q_out, tmp_path))\n",
    "        workers = [asyncio.create_task(fetch_worker(session, q_in, q_out)) for _ in range(FETCH_WORKERS)]\n",
    "\n",
    "        try:\n",
    "            for a in authors_batch:\n",
    "                q_in.put_nowait(a)\n",
    "            for _ in workers:\n",
    "                q_in.put_nowait(None)\n",
    "\n",
    "            await q_in.join()\n",
    "            await q_out.put(None)\n",
    "            await q_out.join()\n",
    "\n",
    "            await asyncio.gather(*workers)\n",
    "            rows_written = await writer_task\n",
    "\n",
    "            # atomic commit\n",
    "            tmp_path.rename(out_path)\n",
    "            return rows_written\n",
    "\n",
    "        finally:\n",
    "            # ensure cleanup on interruption\n",
    "            for t in workers:\n",
    "                t.cancel()\n",
    "            writer_task.cancel()\n",
    "            await asyncio.gather(*workers, return_exceptions=True)\n",
    "            await asyncio.gather(writer_task, return_exceptions=True)\n",
    "\n",
    "# MAIN\n",
    "async def main():\n",
    "    authors = []\n",
    "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                p = loads_line(line)\n",
    "                if p.get(\"works_count\", 0) > 0 and p.get(\"works_api_url\"):\n",
    "                    authors.append(p)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    n = len(authors)\n",
    "    nb = math.ceil(n / BATCH_SIZE)\n",
    "    print(f\"Authors: {n:,} | Batch size: {BATCH_SIZE} | Batches: {nb}\")\n",
    "\n",
    "    t0 = perf_counter()\n",
    "    total_rows = 0\n",
    "\n",
    "    for b in range(nb):\n",
    "        s = b * BATCH_SIZE\n",
    "        e = min((b + 1) * BATCH_SIZE, n)\n",
    "        batch = authors[s:e]\n",
    "\n",
    "        bt0 = perf_counter()\n",
    "        rows_written = await run_batch(b, batch)\n",
    "        dt = perf_counter() - bt0\n",
    "\n",
    "        total_rows += rows_written\n",
    "        print(f\"[batch {b:05d}] authors {s:,}-{e-1:,} | rows {rows_written:,} | {dt/60:.1f} min\")\n",
    "\n",
    "    print(f\"Done. Total rows written (new): {total_rows:,}. Total time: {(perf_counter()-t0)/60:.1f} min\")\n",
    "    print(f\"Parquet parts in: {OUT_DIR.resolve()}\")\n",
    "\n",
    "\n",
    "await main()\n",
    "#   asyncio.run(main())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f4590a",
   "metadata": {},
   "source": [
    "Exploring the parqueet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "578ef37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows:      2,024,311\n",
      "Unique authors:  17,207\n",
      "Unique works:    1,730,232\n",
      "\n",
      "--- Fields in work_json ---\n",
      "  id: https://openalex.org/W2567026504\n",
      "  doi: https://doi.org/10.1016/j.petrol.2016.12.023\n",
      "  title: An efficient two-scale hybrid embedded fracture model for shale gas simulation\n",
      "  display_name: An efficient two-scale hybrid embedded fracture model for shale gas simulation\n",
      "  publication_year: 2016\n",
      "  publication_date: 2016-12-27\n",
      "  ids: dict with keys ['openalex', 'doi', 'mag']\n",
      "  language: en\n",
      "  primary_location: dict with keys ['id', 'is_oa', 'landing_page_url', 'pdf_url', 'source', 'license', 'license_id', 'version', 'is_accepted', 'is_published', 'raw_source_name', 'raw_type']\n",
      "  type: article\n",
      "  indexed_in: list[1]\n",
      "  open_access: dict with keys ['is_oa', 'oa_status', 'oa_url', 'any_repository_has_fulltext']\n",
      "  authorships: list[2]\n",
      "  institutions: list[0]\n",
      "  countries_distinct_count: 1\n",
      "  institutions_distinct_count: 2\n",
      "  corresponding_author_ids: list[1]\n",
      "  corresponding_institution_ids: list[1]\n",
      "  apc_list: None\n",
      "  apc_paid: None\n",
      "  fwci: 2.43454486\n",
      "  has_fulltext: False\n",
      "  cited_by_count: 16\n",
      "  citation_normalized_percentile: dict with keys ['value', 'is_in_top_1_percent', 'is_in_top_10_percent']\n",
      "  cited_by_percentile_year: dict with keys ['min', 'max']\n",
      "  biblio: dict with keys ['volume', 'issue', 'first_page', 'last_page']\n",
      "  is_retracted: False\n",
      "  is_paratext: False\n",
      "  is_xpac: False\n",
      "  primary_topic: dict with keys ['id', 'display_name', 'score', 'subfield', 'field', 'domain']\n",
      "  topics: list[3]\n",
      "  keywords: list[22]\n",
      "  concepts: list[26]\n",
      "  mesh: list[0]\n",
      "  locations_count: 2\n",
      "  locations: list[2]\n",
      "  best_oa_location: None\n",
      "  sustainable_development_goals: list[0]\n",
      "  awards: list[0]\n",
      "  funders: list[0]\n",
      "  has_content: dict with keys ['grobid_xml', 'pdf']\n",
      "  content_urls: None\n",
      "  referenced_works_count: 41\n",
      "  referenced_works: list[41]\n",
      "  related_works: list[10]\n",
      "  abstract_inverted_index: None\n",
      "  counts_by_year: list[7]\n",
      "  updated_date: 2025-11-06T03:46:38.306776\n",
      "  created_date: 2025-10-10T00:00:00\n",
      "\n",
      "--- Sample authorships ---\n",
      "{\n",
      "  \"author_position\": \"first\",\n",
      "  \"author\": {\n",
      "    \"id\": \"https://openalex.org/A5073808531\",\n",
      "    \"display_name\": \"Sahar Z. Amir\",\n",
      "    \"orcid\": \"https://orcid.org/0000-0002-8446-3875\"\n",
      "  },\n",
      "  \"institutions\": [\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/I71920554\",\n",
      "      \"display_name\": \"King Abdullah University of Science and Technology\",\n",
      "      \"ror\": \"https://ror.org/01q3tbs38\",\n",
      "      \"country_code\": \"SA\",\n",
      "      \"type\": \"education\",\n",
      "      \"lineage\": [\n",
      "        \"https://openalex.org/I71920554\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"countries\": [\n",
      "    \"SA\"\n",
      "  ],\n",
      "  \"is_corresponding\": true,\n",
      "  \"raw_author_name\": \"Sahar Z. Amir\",\n",
      "  \"raw_affiliation_strings\": [\n",
      "    \"King Abdullah University of Science and Technology (KAUST), Computational Transport Phenomena Laboratory, and, Physical Science and Engineering Division (PSE), Thuwal 23955-6900, Saudi Arabia\"\n",
      "  ],\n",
      "  \"affiliations\": [\n",
      "    {\n",
      "      \"raw_affiliation_string\": \"King Abdullah University of Science and Technology (KAUST), Computational Transport Phenomena Laboratory, and, Physical Science and Engineering Division (PSE), Thuwal 23955-6900, Saudi Arabia\",\n",
      "      \"institution_ids\": [\n",
      "        \"https://openalex.org/I71920554\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "{\n",
      "  \"author_position\": \"last\",\n",
      "  \"author\": {\n",
      "    \"id\": \"https://openalex.org/A5057212913\",\n",
      "    \"display_name\": \"Shuyu Sun\",\n",
      "    \"orcid\": \"https://orcid.org/0000-0002-3078-864X\"\n",
      "  },\n",
      "  \"institutions\": [\n",
      "    {\n",
      "      \"id\": \"https://openalex.org/I71920554\",\n",
      "      \"display_name\": \"King Abdullah University of Science and Technology\",\n",
      "      \"ror\": \"https://ror.org/01q3tbs38\",\n",
      "      \"country_code\": \"SA\",\n",
      "      \"type\": \"education\",\n",
      "      \"lineage\": [\n",
      "        \"https://openalex.org/I71920554\"\n",
      "      ]\n",
      "    }\n",
      "  ],\n",
      "  \"countries\": [\n",
      "    \"SA\"\n",
      "  ],\n",
      "  \"is_corresponding\": false,\n",
      "  \"raw_author_name\": \"Shuyu Sun\",\n",
      "  \"raw_affiliation_strings\": [\n",
      "    \"King Abdullah University of Science and Technology (KAUST), Computational Transport Phenomena Laboratory, and, Physical Science and Engineering Division (PSE), Thuwal 23955-6900, Saudi Arabia\"\n",
      "  ],\n",
      "  \"affiliations\": [\n",
      "    {\n",
      "      \"raw_affiliation_string\": \"King Abdullah University of Science and Technology (KAUST), Computational Transport Phenomena Laboratory, and, Physical Science and Engineering Division (PSE), Thuwal 23955-6900, Saudi Arabia\",\n",
      "      \"institution_ids\": [\n",
      "        \"https://openalex.org/I71920554\"\n",
      "      ]\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import duckdb\n",
    "from pathlib import Path\n",
    "\n",
    "PARQUET_DIR = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\data\\openalex_works_full_parquet\")\n",
    "\n",
    "con = duckdb.connect()\n",
    "\n",
    "# --- 1. Basic counts ---\n",
    "stats = con.execute(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT author_id) as unique_authors,\n",
    "        COUNT(DISTINCT work_id) as unique_works\n",
    "    FROM read_parquet('{PARQUET_DIR}/*.parquet')\n",
    "\"\"\").fetchone()\n",
    "\n",
    "print(f\"Total rows:      {stats[0]:,}\")\n",
    "print(f\"Unique authors:  {stats[1]:,}\")\n",
    "print(f\"Unique works:    {stats[2]:,}\")\n",
    "\n",
    "# --- 2. Look at one work_json to understand the structure ---\n",
    "sample = con.execute(f\"\"\"\n",
    "    SELECT work_json \n",
    "    FROM read_parquet('{PARQUET_DIR}/*.parquet')\n",
    "    LIMIT 1\n",
    "\"\"\").fetchone()[0]\n",
    "\n",
    "import json\n",
    "work = json.loads(sample)\n",
    "print(\"\\n--- Fields in work_json ---\")\n",
    "for key in work.keys():\n",
    "    val = work[key]\n",
    "    if isinstance(val, list):\n",
    "        print(f\"  {key}: list[{len(val)}]\")\n",
    "    elif isinstance(val, dict):\n",
    "        print(f\"  {key}: dict with keys {list(val.keys())}\")\n",
    "    elif isinstance(val, str) and len(val) > 80:\n",
    "        print(f\"  {key}: '{val[:80]}...'\")\n",
    "    else:\n",
    "        print(f\"  {key}: {val}\")\n",
    "\n",
    "# --- 3. Check what 'authorships' looks like inside work_json ---\n",
    "print(\"\\n--- Sample authorships ---\")\n",
    "for authorship in work.get(\"authorships\", [])[:3]:\n",
    "    print(json.dumps(authorship, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f4bf2b",
   "metadata": {},
   "source": [
    "===================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d133205",
   "metadata": {},
   "source": [
    "#### Trajectories\n",
    "Finding ICCS Conference papers in JOCS Journal or vice versa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee2b841c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Loaded 8302 ICCS papers and 2077 JoCS papers.\n",
      "Found 134 ICCS papers that went to JoCS.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def load_jsonl(path):\n",
    "    data = []\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            rec = json.loads(line)\n",
    "            w = rec.get('openalex_work', {})\n",
    "            # Extract basic fields\n",
    "            authors = [a['author']['id'] for a in w.get('authorships', []) if 'author' in a]\n",
    "            data.append({\n",
    "                'id': w.get('id'),\n",
    "                'title': w.get('display_name'),\n",
    "                'year': w.get('publication_year'),\n",
    "                'authors': set(authors),\n",
    "                'citations': w.get('cited_by_count')\n",
    "            })\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Paths (Aapke wale)\n",
    "iccs_path = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\Data\\processed\\pro_oa_iccs\\iccs_final_complete_authors.jsonl\"\n",
    "jocs_path = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\Data\\processed\\pro_oa_jocs\\jocs_final_complete_authors.jsonl\"\n",
    "\n",
    "print(\"Loading data...\")\n",
    "iccs_df = load_jsonl(iccs_path)\n",
    "jocs_df = load_jsonl(jocs_path)\n",
    "print(f\"Loaded {len(iccs_df)} ICCS papers and {len(jocs_df)} JoCS papers.\")\n",
    "\n",
    "# 2. Matching Logic\n",
    "matches = []\n",
    "for idx, iccs in iccs_df.iterrows():\n",
    "    # Only look at JoCS papers published AFTER ICCS paper (up to 3 years)\n",
    "    candidates = jocs_df[\n",
    "        (jocs_df['year'] >= iccs['year']) & \n",
    "        (jocs_df['year'] <= iccs['year'] + 3)\n",
    "    ]\n",
    "    \n",
    "    for _, jocs in candidates.iterrows():\n",
    "        # Fast Check: Author Overlap\n",
    "        if not iccs['authors'].intersection(jocs['authors']):\n",
    "            continue\n",
    "            \n",
    "        # Slow Check: Title Similarity\n",
    "        sim = SequenceMatcher(None, str(iccs['title']).lower(), str(jocs['title']).lower()).ratio()\n",
    "        \n",
    "        if sim > 0.6: # Threshold\n",
    "            matches.append({\n",
    "                'iccs_id': iccs['id'],\n",
    "                'jocs_id': jocs['id'],\n",
    "                'similarity': sim,\n",
    "                'jocs_citations': jocs['citations']\n",
    "            })\n",
    "\n",
    "match_df = pd.DataFrame(matches).sort_values('similarity', ascending=False).drop_duplicates('iccs_id')\n",
    "print(f\"Found {len(match_df)} ICCS papers that went to JoCS.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
