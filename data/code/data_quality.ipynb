{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5934c84d",
   "metadata": {},
   "source": [
    "# JOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b79e3",
   "metadata": {},
   "source": [
    "Grobid Data Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de24ec37",
   "metadata": {},
   "source": [
    "### Cross-Check Trash DOIs vs Final JSONL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171585a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking 53 Trash DOIs against final JSONL...\n",
      "\n",
      "============================================================\n",
      "ðŸ“Š DATA INTEGRITY REPORT\n",
      "------------------------------------------------------------\n",
      "Total DOIs in Final JSONL: 2077\n",
      "Trash DOIs checked:        53\n",
      "\n",
      "ðŸš© ALERT: 40 files abhi bhi final data mein baithi hain!\n",
      "  -> Found: 10.1016/j.jocs.2024.102379\n",
      "  -> Found: 10.1016/j.jocs.2023.102183\n",
      "  -> Found: 10.1016/j.jocs.2023.102102\n",
      "  -> Found: 10.1016/j.jocs.2022.101745\n",
      "  -> Found: 10.1016/j.jocs.2020.101227\n",
      "  -> Found: 10.1016/j.jocs.2020.101258\n",
      "  -> Found: 10.1016/j.jocs.2021.101395\n",
      "  -> Found: 10.1016/j.jocs.2020.101222\n",
      "  -> Found: 10.1016/j.jocs.2020.101187\n",
      "  -> Found: 10.1016/j.jocs.2020.101168\n",
      "  -> Found: 10.1016/j.jocs.2019.05.004\n",
      "  -> Found: 10.1016/j.jocs.2019.04.006\n",
      "  -> Found: 10.1016/j.jocs.2019.05.006\n",
      "  -> Found: 10.1016/j.jocs.2019.05.005\n",
      "  -> Found: 10.1016/j.jocs.2019.03.002\n",
      "  -> Found: 10.1016/j.jocs.2017.11.014\n",
      "  -> Found: 10.1016/j.jocs.2018.08.010\n",
      "  -> Found: 10.1016/j.jocs.2018.04.014\n",
      "  -> Found: 10.1016/j.jocs.2018.02.005\n",
      "  -> Found: 10.1016/j.jocs.2018.04.007\n",
      "  -> Found: 10.1016/j.jocs.2017.10.004\n",
      "  -> Found: 10.1016/j.jocs.2017.05.020\n",
      "  -> Found: 10.1016/j.jocs.2016.02.004\n",
      "  -> Found: 10.1016/j.jocs.2016.02.007\n",
      "  -> Found: 10.1016/j.jocs.2015.08.007\n",
      "  -> Found: 10.1016/j.jocs.2015.06.004\n",
      "  -> Found: 10.1016/j.jocs.2015.10.002\n",
      "  -> Found: 10.1016/j.jocs.2015.06.005\n",
      "  -> Found: 10.1016/j.jocs.2015.08.006\n",
      "  -> Found: 10.1016/j.jocs.2014.02.001\n",
      "  -> Found: 10.1016/j.jocs.2014.04.001\n",
      "  -> Found: 10.1016/j.jocs.2014.04.009\n",
      "  -> Found: 10.1016/j.jocs.2014.06.009\n",
      "  -> Found: 10.1016/j.jocs.2013.01.001\n",
      "  -> Found: 10.1016/j.jocs.2013.08.001\n",
      "  -> Found: 10.1016/j.jocs.2012.12.001\n",
      "  -> Found: 10.1016/j.jocs.2012.04.001\n",
      "  -> Found: 10.1016/j.jocs.2012.08.009\n",
      "  -> Found: 10.1016/j.jocs.2012.07.001\n",
      "  -> Found: 10.1016/j.jocs.2011.11.001\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Paths\n",
    "trash_csv = \"jocs_trash_files_only.csv\"\n",
    "jsonl_file = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\jocs_final_complete_authors.jsonl\")\n",
    "\n",
    "def fix_filename_to_doi(filename):\n",
    "    # 1. Extension (.json) hatao\n",
    "    name = filename.replace('.json', '')\n",
    "    \n",
    "    # 2. Year prefix (e.g., 2010_) hatao\n",
    "    # Hum pehla underscore dhoond kar uske baad ka hissa lenge\n",
    "    if '_' in name:\n",
    "        parts = name.split('_', 1)\n",
    "        doi_part = parts[1] # e.g., '10.1016_j.jocs.2010.04.003'\n",
    "        \n",
    "        # 3. Pehle underscore ko forward slash se badlo\n",
    "        # Result: '10.1016/j.jocs.2010.04.003'\n",
    "        normalized = doi_part.replace('_', '/', 1)\n",
    "        return normalized\n",
    "    return name\n",
    "\n",
    "def verify_cleanup():\n",
    "    if not os.path.exists(trash_csv) or not jsonl_file.exists():\n",
    "        print(\"âŒ Files missing! Check paths.\")\n",
    "        return\n",
    "\n",
    "    # CSV load karo\n",
    "    trash_df = pd.read_csv(trash_csv)\n",
    "    trash_dois = set(trash_df['filename'].apply(fix_filename_to_doi))\n",
    "    \n",
    "    print(f\"Checking {len(trash_dois)} Trash DOIs against final JSONL...\")\n",
    "\n",
    "    matches = []\n",
    "    total_jsonl_count = 0\n",
    "\n",
    "    # JSONL check\n",
    "    with open(jsonl_file, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            total_jsonl_count += 1\n",
    "            try:\n",
    "                data = json.loads(line)\n",
    "                # normalized_doi check kar rahe hain\n",
    "                file_doi = data.get('normalized_doi', \"\").strip()\n",
    "                \n",
    "                if file_doi in trash_dois:\n",
    "                    matches.append(file_doi)\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    # --- REPORT ---\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(f\"DATA INTEGRITY REPORT\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"Total DOIs in Final JSONL: {total_jsonl_count}\")\n",
    "    print(f\"Trash DOIs checked:        {len(trash_dois)}\")\n",
    "    \n",
    "    if not matches:\n",
    "        print(\"\\nMUBARAK HO! Logic fix ke baad bhi koi trash match nahi mila.\")\n",
    "        print(\"Iska matlab 'Situation 8' wali kachra files already filter out ho chuki hain.\")\n",
    "    else:\n",
    "        print(f\"\\nðŸš© ALERT: {len(matches)} files abhi bhi final data mein baithi hain!\")\n",
    "        for m in matches:\n",
    "            print(f\"  -> Found: {m}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    verify_cleanup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb57d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Path Configuration\n",
    "INPUT_CSV = \"jocs_trash_files_only.csv\"\n",
    "INPUT_JSONL = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\jocs_final_complete_authors.jsonl\")\n",
    "OUTPUT_JSONL = INPUT_JSONL.parent / \"final_jocs_openalex.jsonl\"\n",
    "\n",
    "def normalize_doi_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Converts filename format to normalized DOI format.\n",
    "    Example: '2010_10.1016_j.jocs.2010.04.003.json' -> '10.1016/j.jocs.2010.04.003'\n",
    "    \"\"\"\n",
    "    # Remove file extension\n",
    "    name_no_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    if '_' in name_no_ext:\n",
    "        # Split at the first underscore to remove the year prefix\n",
    "        _, doi_part = name_no_ext.split('_', 1)\n",
    "        # Replace the first underscore in the remaining string with a forward slash\n",
    "        normalized_doi = doi_part.replace('_', '/', 1)\n",
    "        return normalized_doi\n",
    "    \n",
    "    return name_no_ext\n",
    "\n",
    "def generate_final_dataset():\n",
    "    \"\"\"\n",
    "    Filters the input JSONL file by removing records present in the exclusion CSV.\n",
    "    Writes the resulting clean data to a new JSONL file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_CSV):\n",
    "        print(f\"Error: Required exclusion file '{INPUT_CSV}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not INPUT_JSONL.exists():\n",
    "        print(f\"Error: Input dataset '{INPUT_JSONL}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load exclusion list into a set for O(1) lookup performance\n",
    "    try:\n",
    "        df_trash = pd.read_csv(INPUT_CSV)\n",
    "        exclusion_set = set(df_trash['filename'].apply(normalize_doi_from_filename))\n",
    "        print(f\"Exclusion set initialized with {len(exclusion_set)} DOIs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading exclusion CSV: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    records_removed = 0\n",
    "    records_retained = 0\n",
    "\n",
    "    # Stream process the JSONL to handle large data volumes\n",
    "    try:\n",
    "        with open(INPUT_JSONL, 'r', encoding='utf-8') as f_in, \\\n",
    "             open(OUTPUT_JSONL, 'w', encoding='utf-8') as f_out:\n",
    "            \n",
    "            for line_number, line in enumerate(f_in, 1):\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    doi = record.get('normalized_doi', \"\").strip()\n",
    "                    \n",
    "                    if doi in exclusion_set:\n",
    "                        records_removed += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Write clean records back to the new file\n",
    "                    f_out.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "                    records_retained += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping malformed JSON on line {line_number}\")\n",
    "                    continue\n",
    "\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"DATASET PURIFICATION COMPLETE\")\n",
    "        print(\"-\" * 60)\n",
    "        print(f\"Output File:      {OUTPUT_JSONL.name}\")\n",
    "        print(f\"Records Excluded: {records_removed}\")\n",
    "        print(f\"Records Retained: {records_retained}\")\n",
    "        print(f\"Full Path:        {OUTPUT_JSONL}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file processing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_final_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b936fb63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for 'Only Title' files (Skipping files from jocs_trash_files_only.csv)...\n",
      "\n",
      "âœ… Koi aisi file nahi mili jo sirf title wali ho (aur trash mein na ho).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "from pathlib import Path\n",
    "\n",
    "# Setup Paths\n",
    "parsed_path = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\parsed\\jocs_grobid\\all\")\n",
    "trash_csv = \"jocs_trash_files_only.csv\"\n",
    "\n",
    "def print_only_title_files(folder, skip_csv):\n",
    "    # 1. CSV se skip hone wali files ki list banao\n",
    "    skip_list = set()\n",
    "    if os.path.exists(skip_csv):\n",
    "        with open(skip_csv, \"r\", encoding=\"utf-8\") as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                skip_list.add(row['filename'])\n",
    "    \n",
    "    found_files = []\n",
    "\n",
    "    print(f\"Scanning for 'Only Title' files (Skipping files from {skip_csv})...\\n\")\n",
    "\n",
    "    for file_path in folder.glob(\"*.json\"):\n",
    "        # Agar file CSV mein hai, toh skip karo\n",
    "        if file_path.name in skip_list:\n",
    "            continue\n",
    "            \n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            try:\n",
    "                data = json.load(f)\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "            # Logic: Title True, Abstract False, Keywords False\n",
    "            title_text = str(data.get('title') or \"\").strip()\n",
    "            has_title = bool(title_text)\n",
    "            has_kywd = bool(data.get('keywords'))\n",
    "            has_abst = bool(str(data.get('abstract') or \"\").strip())\n",
    "\n",
    "            if has_title and not has_kywd and not has_abst:\n",
    "                found_files.append({\n",
    "                    \"name\": file_path.name,\n",
    "                    \"title\": title_text[:80] + \"...\" if len(title_text) > 80 else title_text\n",
    "                })\n",
    "\n",
    "    # --- RESULTS PRINTING ---\n",
    "    if not found_files:\n",
    "        print(\"âœ… Koi aisi file nahi mili jo sirf title wali ho (aur trash mein na ho).\")\n",
    "    else:\n",
    "        print(f\"ðŸš€ Found {len(found_files)} files with ONLY TITLE:\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"{'Filename':<40} | {'Extracted Title'}\")\n",
    "        print(\"-\" * 100)\n",
    "        for file in found_files:\n",
    "            print(f\"{file['name']:<40} | {file['title']}\")\n",
    "        print(\"-\" * 100)\n",
    "        print(f\"Total: {len(found_files)} files.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print_only_title_files(parsed_path, trash_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f25226e",
   "metadata": {},
   "source": [
    "Docling Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70262901",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning JOCS folder...\n",
      "Success! Loaded 2094 papers. 1990 are high quality.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "jocs_folder = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\parsed\\jocs\\all\")\n",
    "output_dir = Path(\"jocs_analysis\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Scanning JOCS folder...\")\n",
    "all_files = list(jocs_folder.glob(\"*.json\"))\n",
    "\n",
    "papers = []\n",
    "for json_file in all_files:\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # 1. Extraction with Year Fix\n",
    "            # Filename starts with \"2010_\", so we split by underscore and take the first part\n",
    "            filename_year = json_file.name.split('_')[0]\n",
    "            \n",
    "            paper_info = {\n",
    "                'filename': json_file.name,\n",
    "                'year': int(filename_year) if filename_year.isdigit() else 0,\n",
    "                'title': data.get('title', ''),\n",
    "                'abstract': data.get('abstract', ''),\n",
    "                'keywords': data.get('keywords', ''),\n",
    "                # 'author_count': len(data.get('authors', [])) # Bonus: keep track of team size\n",
    "            }\n",
    "            \n",
    "            # Combine for NLP/ML analysis\n",
    "            paper_info['full_text'] = f\"{paper_info['title']} {paper_info['abstract']} {paper_info['keywords']}\"\n",
    "            \n",
    "            papers.append(paper_info)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {json_file.name}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "# --- CLEANING LOGIC ---\n",
    "# Usable papers: Needs a title and a meaningful abstract\n",
    "usable_mask = (df['title'].str.len() > 10) & (df['abstract'].str.len() > 30)\n",
    "df_clean = df[usable_mask].copy()\n",
    "\n",
    "# --- SAVE ---\n",
    "df_clean.to_csv(output_dir / \"jocs_docling_cleaned.csv\", index=False)\n",
    "print(f\"Success! Loaded {len(df)} papers. {len(df_clean)} are high quality.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cffdc1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MISSING DATA AUDIT\n",
      "Title     :   67 papers missing (  3.2%)\n",
      "Keywords  :   90 papers missing (  4.3%)\n",
      "Abstract  :   63 papers missing (  3.0%)\n",
      "Total Completely Empty: 25 papers\n"
     ]
    }
   ],
   "source": [
    "fields = ['title', 'keywords', 'abstract']\n",
    "\n",
    "print(\"MISSING DATA AUDIT\")\n",
    "\n",
    "for field in fields:\n",
    "    # Check for both actual NaNs and empty strings/whitespace\n",
    "    is_missing = df[field].isna() | (df[field].astype(str).str.strip() == \"\")\n",
    "    missing_count = is_missing.sum()\n",
    "    missing_pct = (missing_count / len(df)) * 100\n",
    "    \n",
    "    print(f\"{field.capitalize():<10}: {missing_count:>4} papers missing ({missing_pct:>5.1f}%)\")\n",
    "\n",
    "# Check for \"Completely Empty\" rows (missing all three)\n",
    "all_missing = (\n",
    "    (df['title'].isna() | (df['title'].astype(str).str.strip() == \"\")) &\n",
    "    (df['keywords'].isna() | (df['keywords'].astype(str).str.strip() == \"\")) &\n",
    "    (df['abstract'].isna() | (df['abstract'].astype(str).str.strip() == \"\"))\n",
    ").sum()\n",
    "\n",
    "\n",
    "print(f\"Total Completely Empty: {all_missing} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a33e2b5",
   "metadata": {},
   "source": [
    "Filling the missing data from both dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4a8fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "\n",
      "Scanning DOCLING for Smushed Text...\n",
      "Filename / DOI                                     | Space %  | Snippet (First 80 chars)\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "2024_10.1016_j.jocs.2024.102466.json               | 3.91%   | Dataset link:https://github.com/AGN000/PIN Addressingdiscontinuitiesinfluidflowp...\n",
      "2024_10.1016_j.jocs.2024.102469.json               | 4.97%   | Keywords: WestudythetransitiontophasesynchronizationinanensembleofStuartâ€“Landauo...\n",
      "2025_10.1016_j.jocs.2024.102468.json               | 4.49%   | Keywords: Black-boxauto-tuningmethodshavebeenproventobeefficientfortuningconfigu...\n",
      "2025_10.1016_j.jocs.2024.102473.json               | 2.73%   | Keywords: Identifying influential nodes is crucial in network science for contro...\n",
      "2025_10.1016_j.jocs.2024.102498.json               | 2.41%   | Keywords: Theenhancedcapabilitiesofautonomousunderwatervehicles(AUVs)willfacilit...\n",
      "2025_10.1016_j.jocs.2025.102527.json               | 3.58%   | Keywords: Theincreasingflowofenvironmentalpoisonoussubstancesintoaquaticsystemse...\n",
      "2025_10.1016_j.jocs.2025.102535.json               | 4.19%   | Keywords: Extensive work has been done in supervised continual learning (SCL) , ...\n",
      "2025_10.1016_j.jocs.2025.102539.json               | 1.46%   | Keywords: ThispaperintroducestheInterpointInceptionDistance(IID)asanewapproachfo...\n",
      "------------------------------------------------------------------------------------------------------------------------\n",
      "REPORT FOR DOCLING:\n",
      "Total Abstracts Checked: 1990\n",
      "Smushed/Corrupted Found: 8 (0.40%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 1. Load Data\n",
    "# Apne paths confirm kar lena\n",
    "docling_path = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\jocs_analysis\\jocs_docling_cleaned.csv\"\n",
    "grobid_path = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\jocs_analysis\\jocs_grobid_cleaned.csv\"\n",
    "\n",
    "# Load CSVs\n",
    "print(\"Loading datasets...\")\n",
    "df_doc = pd.read_csv(docling_path)\n",
    "df_grob = pd.read_csv(grobid_path)\n",
    "\n",
    "def check_smushed_abstracts(df, source_name, threshold=0.05):\n",
    "    print(f\"\\nScanning {source_name} for Smushed Text...\")\n",
    "    print(f\"{'Filename / DOI':<50} | {'Space %':<8} | {'Snippet (First 80 chars)'}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    bad_count = 0\n",
    "    total_checked = 0\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        text = str(row.get('abstract', ''))\n",
    "        \n",
    "        # Sirf tab check karo agar text maujood hai aur \"nan\" nahi hai\n",
    "        if text and text.lower() != 'nan' and len(text) > 50:\n",
    "            total_checked += 1\n",
    "            \n",
    "            # Logic: Count spaces vs Total Length\n",
    "            space_count = text.count(' ')\n",
    "            ratio = space_count / len(text)\n",
    "            \n",
    "            # Agar spaces 5% se kam hain (Yani 100 letters mein 5 se kam spaces)\n",
    "            if ratio < threshold:\n",
    "                bad_count += 1\n",
    "                # Filename ya DOI jo bhi available ho\n",
    "                ide = str(row.get('doi', row.get('filename', 'Unknown')))[:45]\n",
    "                snippet = text[:80].replace('\\n', ' ') # Print ke liye newline hata dein\n",
    "                \n",
    "                print(f\"{ide:<50} | {ratio:.2%}   | {snippet}...\")\n",
    "    \n",
    "    print(\"-\" * 120)\n",
    "    print(f\"REPORT FOR {source_name}:\")\n",
    "    print(f\"Total Abstracts Checked: {total_checked}\")\n",
    "    print(f\"Smushed/Corrupted Found: {bad_count} ({(bad_count/total_checked)*100:.2f}%)\")\n",
    "\n",
    "# 2. Run Audit on Docling (Main Culprit)\n",
    "check_smushed_abstracts(df_doc, \"DOCLING\")\n",
    "\n",
    "# 3. Optional: Run on Grobid just to be sure\n",
    "# check_smushed_abstracts(df_grob, \"GROBID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0c5f5387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repairing Titles, Abstracts, and Keywords...\n",
      "\n",
      "MERGE COMPLETE\n",
      "Total Papers in Master Dataset: 2026\n",
      "Repaired Year Range: 2010 - 2025\n",
      "Master file saved to: D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\jocs_analysis\\jocs_master_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load your two datasets\n",
    "df_docling = pd.read_csv(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\jocs_analysis\\jocs_docling_cleaned.csv\")\n",
    "df_grobid = pd.read_csv(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\jocs_analysis\\jocs_grobid_cleaned.csv\")\n",
    "\n",
    "# 2. Perform an \"Outer Join\" on filename to include every paper found by either parser\n",
    "df_combined = pd.merge(df_grobid, df_docling, on='filename', how='outer', suffixes=('_g', '_d'))\n",
    "\n",
    "# Helper function to pick the \"Better\" (longer/non-null) string\n",
    "def pick_best(val_g, val_d):\n",
    "    # Convert to string and handle NaNs/None\n",
    "    s_g = str(val_g).strip() if pd.notna(val_g) else \"\"\n",
    "    s_d = str(val_d).strip() if pd.notna(val_d) else \"\"\n",
    "    \n",
    "    # If one is empty, return the other. If both have text, return the longest one.\n",
    "    if not s_g: return s_d\n",
    "    if not s_d: return s_g\n",
    "    return s_g if len(s_g) >= len(s_d) else s_d\n",
    "\n",
    "# 3. Apply the \"Mutual Healing\" logic to all key fields\n",
    "print(\"Repairing Titles, Abstracts, and Keywords...\")\n",
    "df_combined['final_title'] = df_combined.apply(lambda x: pick_best(x['title_g'], x['title_d']), axis=1)\n",
    "df_combined['final_abstract'] = df_combined.apply(lambda x: pick_best(x['abstract_g'], x['abstract_d']), axis=1)\n",
    "df_combined['final_keywords'] = df_combined.apply(lambda x: pick_best(x['keywords_g'], x['keywords_d']), axis=1)\n",
    "\n",
    "# 4. Repair the Year (Take either, prioritizing Grobid's extraction if valid)\n",
    "df_combined['final_year'] = df_combined['year_g'].fillna(df_combined['year_d']).replace(0, np.nan)\n",
    "\n",
    "# 5. Final Cleanup\n",
    "df_final = df_combined[[\n",
    "    'filename', \n",
    "    'final_title', \n",
    "    'final_abstract', \n",
    "    'final_keywords', \n",
    "    'final_year'\n",
    "]].copy()\n",
    "\n",
    "df_final.columns = ['filename', 'title', 'abstract', 'keywords', 'year']\n",
    "\n",
    "# 6. Final Filter: Remove rows where both title AND abstract are still missing (the 'completely empty' ones)\n",
    "df_final = df_final.dropna(subset=['title', 'abstract'], how='all')\n",
    "\n",
    "# Output Results\n",
    "print(f\"\\nMERGE COMPLETE\")\n",
    "print(f\"Total Papers in Master Dataset: {len(df_final)}\")\n",
    "print(f\"Repaired Year Range: {int(df_final['year'].min())} - {int(df_final['year'].max())}\")\n",
    "\n",
    "# Save the final masterpiece\n",
    "output_path = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\jocs_analysis\\jocs_master_dataset.csv\"\n",
    "df_final.to_csv(output_path, index=False)\n",
    "print(f\"Master file saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8710daf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FINAL MASTER DATASET AUDIT\n",
      "----------------------------------------\n",
      "Title     :    0 papers missing (  0.0%)\n",
      "Keywords  :   25 papers missing (  1.2%)\n",
      "Abstract  :    0 papers missing (  0.0%)\n",
      "Total Completely Empty : 0 papers\n",
      "Total Usable for ML    : 2026 papers (Title + Abstract)\n"
     ]
    }
   ],
   "source": [
    "df_audit = df_final \n",
    "\n",
    "# Define the fields to check\n",
    "fields = ['title', 'keywords', 'abstract']\n",
    "\n",
    "print(\"FINAL MASTER DATASET AUDIT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for field in fields:\n",
    "    # Check for both actual NaNs and empty strings/whitespace\n",
    "    # We use .fillna('') first to ensure the string strip doesn't fail on NaNs\n",
    "    is_missing = df_audit[field].fillna('').astype(str).str.strip() == \"\"\n",
    "    missing_count = is_missing.sum()\n",
    "    missing_pct = (missing_count / len(df_audit)) * 100\n",
    "    \n",
    "    print(f\"{field.capitalize():<10}: {missing_count:>4} papers missing ({missing_pct:>5.1f}%)\")\n",
    "\n",
    "# 1. Check for \"Completely Empty\" (missing all three metadata fields)\n",
    "all_missing_mask = (\n",
    "    (df_audit['title'].fillna('').str.strip() == \"\") &\n",
    "    (df_audit['keywords'].fillna('').str.strip() == \"\") &\n",
    "    (df_audit['abstract'].fillna('').str.strip() == \"\")\n",
    ")\n",
    "all_missing_count = all_missing_mask.sum()\n",
    "\n",
    "# 2. Check for \"Usable\" (Has Title AND Abstract)\n",
    "# This is your key metric for Big Data / ML School\n",
    "usable_mask = (\n",
    "    (df_audit['title'].fillna('').str.strip() != \"\") & \n",
    "    (df_audit['abstract'].fillna('').str.strip().str.len() > 50)\n",
    ")\n",
    "usable_count = usable_mask.sum()\n",
    "\n",
    "print(f\"Total Completely Empty : {all_missing_count} papers\")\n",
    "print(f\"Total Usable for ML    : {usable_count} papers (Title + Abstract)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "03698ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of papers saved specifically by Docling: 89\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "filename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "final_title",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "final_abstract",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a7438f86-79d7-4cfd-9b4b-0cc2cfb6e4ab",
       "rows": [
        [
         "3",
         "2010_10.1016_j.jocs.2010.03.004.json",
         "Unstable periodic orbits in weak turbulence",
         "We report on a sophisticated numerical study of a parallel space-time algorithm for the computation of periodic solutions of the driven, incompressible Navier-Stokes equations in the turbulent regime. Efforts to apply the machinery of dynamical systems theory to fluid turbulence depend on the ability to accurately and reliably compute such unstable periodic orbits (UPOs). For example, the UPOs may be used to construct the dynamical zeta function of the system, from which very accurate turbulent averages of observables may be extracted."
        ],
        [
         "9",
         "2010_10.1016_j.jocs.2010.03.011.json",
         "A dynamic wind farm aggregate model for the simulation of power fluctuations due to wind turbulence",
         "Animportantaspect related to wind energy integration into the electrical power system is the fluctuation of the generated power due to the stochastic variations of the wind speed across the area where wind turbines are installed. Simulation models are useful tools to evaluate the impact of the wind power on the power system stability and on the power quality. Aggregate models reduce the simulation time required by detailed dynamic models of multiturbine systems."
        ],
        [
         "72",
         "2012_10.1016_j.jocs.2011.06.003.json",
         "Quadpack computation of Feynman loop integrals",
         "The paper addresses a numerical computation of Feynman loop integrals, which are computed by an extrapolation to the limit as a parameter in the integrand tends to zero. An important objective is to achieve an automatic computation which is effective for a wide range of instances. Singular or near singular integrand behavior is handled via an adaptive partitioning of the domain, implemented in an iterated/repeated multivariate integration method. Integrand singularities possibly introduced via infrared (IR) divergence at the boundaries of the integration domain are addressed using a version of the Dqags algorithm from the integration package Quadpack, which uses an adaptive strategy combined with extrapolation. The latter is justified for a large class of problems by the underlying asymptotic expansions of the integration error. For IR divergent problems, an extrapolation scheme is presented based on dimensional regularization."
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 3
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>final_title</th>\n",
       "      <th>final_abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010_10.1016_j.jocs.2010.03.004.json</td>\n",
       "      <td>Unstable periodic orbits in weak turbulence</td>\n",
       "      <td>We report on a sophisticated numerical study o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2010_10.1016_j.jocs.2010.03.011.json</td>\n",
       "      <td>A dynamic wind farm aggregate model for the si...</td>\n",
       "      <td>Animportantaspect related to wind energy integ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>2012_10.1016_j.jocs.2011.06.003.json</td>\n",
       "      <td>Quadpack computation of Feynman loop integrals</td>\n",
       "      <td>The paper addresses a numerical computation of...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                filename  \\\n",
       "3   2010_10.1016_j.jocs.2010.03.004.json   \n",
       "9   2010_10.1016_j.jocs.2010.03.011.json   \n",
       "72  2012_10.1016_j.jocs.2011.06.003.json   \n",
       "\n",
       "                                          final_title  \\\n",
       "3         Unstable periodic orbits in weak turbulence   \n",
       "9   A dynamic wind farm aggregate model for the si...   \n",
       "72     Quadpack computation of Feynman loop integrals   \n",
       "\n",
       "                                       final_abstract  \n",
       "3   We report on a sophisticated numerical study o...  \n",
       "9   Animportantaspect related to wind energy integ...  \n",
       "72  The paper addresses a numerical computation of...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Look for rows where one parser was empty but the final is full\n",
    "saved_rows = df_combined[df_combined['abstract_g'].isna() & df_combined['final_abstract'].notna()]\n",
    "\n",
    "print(f\"Number of papers saved specifically by Docling: {len(saved_rows)}\")\n",
    "# Display the first 3 examples to see the titles and abstracts\n",
    "display(saved_rows[['filename', 'final_title', 'final_abstract']].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5eb3d9a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "132f64fe",
   "metadata": {},
   "source": [
    "# ICCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac8332d",
   "metadata": {},
   "source": [
    "GROBID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40cfc0d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning ICCS Grobid folder: D:\\ITMO Big Data & ML School\\semester 3\\RI3\\parsed\\iccs\n",
      "\n",
      "[GROBID] Success! Loaded 8324 papers. 8118 are high quality.\n",
      "------------------------------\n",
      "MISSING DATA AUDIT (GROBID)\n",
      "------------------------------\n",
      "Title     :  104 papers missing (  1.2%)\n",
      "Keywords  : 3075 papers missing ( 36.9%)\n",
      "Abstract  :  189 papers missing (  2.3%)\n",
      "Total Completely Empty: 82 papers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# POINTING TO THE ROOT OF THE ICCS GROBID FOLDER\n",
    "iccs_folder = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\parsed\\iccs\")\n",
    "output_dir = Path(\"iccs_analysis\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Scanning ICCS Grobid folder: {iccs_folder}\")\n",
    "# CHANGED: .rglob to find json files inside year subfolders\n",
    "all_files = list(iccs_folder.rglob(\"*.json\"))\n",
    "\n",
    "papers = []\n",
    "for json_file in all_files:\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # 1. Year Logic\n",
    "            # Try to get year from folder name first (e.g. .../2001/file.json)\n",
    "            # If that fails, try filename split\n",
    "            parent_folder = json_file.parent.name\n",
    "            if parent_folder.isdigit():\n",
    "                year = int(parent_folder)\n",
    "            else:\n",
    "                # Fallback to filename parsing\n",
    "                part = json_file.name.split('_')[0]\n",
    "                year = int(part) if part.isdigit() else 0\n",
    "            \n",
    "            paper_info = {\n",
    "                'filename': json_file.name,\n",
    "                'year': year,\n",
    "                'title': data.get('title', ''),\n",
    "                'abstract': data.get('abstract', ''),\n",
    "                'keywords': data.get('keywords', ''),\n",
    "                'source': 'grobid'\n",
    "            }\n",
    "            \n",
    "            # Combine for NLP/ML analysis\n",
    "            paper_info['full_text'] = f\"{paper_info['title']} {paper_info['abstract']} {paper_info['keywords']}\"\n",
    "            \n",
    "            papers.append(paper_info)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {json_file.name}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "# --- CLEANING LOGIC ---\n",
    "usable_mask = (df['title'].str.len() > 10) & (df['abstract'].str.len() > 30)\n",
    "df_clean = df[usable_mask].copy()\n",
    "\n",
    "# --- SAVE ---\n",
    "df_clean.to_csv(output_dir / \"iccs_grobid_cleaned.csv\", index=False)\n",
    "print(f\"\\n[GROBID] Success! Loaded {len(df)} papers. {len(df_clean)} are high quality.\")\n",
    "\n",
    "fields = ['title', 'keywords', 'abstract']\n",
    "print(\"-\" * 30)\n",
    "print(\"MISSING DATA AUDIT (GROBID)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for field in fields:\n",
    "    is_missing = df[field].isna() | (df[field].astype(str).str.strip() == \"\")\n",
    "    missing_count = is_missing.sum()\n",
    "    missing_pct = (missing_count / len(df)) * 100\n",
    "    print(f\"{field.capitalize():<10}: {missing_count:>4} papers missing ({missing_pct:>5.1f}%)\")\n",
    "\n",
    "all_missing = (\n",
    "    (df['title'].astype(str).str.strip() == \"\") &\n",
    "    (df['keywords'].astype(str).str.strip() == \"\") &\n",
    "    (df['abstract'].astype(str).str.strip() == \"\")\n",
    ").sum()\n",
    "\n",
    "print(f\"Total Completely Empty: {all_missing} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "298033b8",
   "metadata": {},
   "source": [
    "DOCLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffe6fa62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Scanning ICCS Docling folder: D:\\ITMO Big Data & ML School\\semester 3\\RI3\\parsed\\icss_docling\n",
      "\n",
      "[DOCLING] Success! Loaded 6135 papers. 5904 are high quality.\n",
      "------------------------------\n",
      "MISSING DATA AUDIT (DOCLING)\n",
      "------------------------------\n",
      "Title     :    2 papers missing (  0.0%)\n",
      "Keywords  : 3048 papers missing ( 49.7%)\n",
      "Abstract  :  199 papers missing (  3.2%)\n",
      "Total Completely Empty: 0 papers\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# POINTING TO THE ROOT OF THE ICCS DOCLING FOLDER\n",
    "# Note: Ensure the path matches your actual folder name (icss_docling vs iccs_docling)\n",
    "docling_folder = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\parsed\\icss_docling\")\n",
    "output_dir = Path(\"iccs_analysis\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"\\nScanning ICCS Docling folder: {docling_folder}\")\n",
    "# CHANGED: .rglob to find json files inside year subfolders\n",
    "all_files = list(docling_folder.rglob(\"*.json\"))\n",
    "\n",
    "papers = []\n",
    "for json_file in all_files:\n",
    "    try:\n",
    "        with open(json_file, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            \n",
    "            # 1. Year Logic (Folder based is safer)\n",
    "            parent_folder = json_file.parent.name\n",
    "            if parent_folder.isdigit():\n",
    "                year = int(parent_folder)\n",
    "            else:\n",
    "                part = json_file.name.split('_')[0]\n",
    "                year = int(part) if part.isdigit() else 0\n",
    "            \n",
    "            paper_info = {\n",
    "                'filename': json_file.name,\n",
    "                'year': year,\n",
    "                'title': data.get('title', ''),\n",
    "                'abstract': data.get('abstract', ''),\n",
    "                'keywords': data.get('keywords', ''),\n",
    "                'source': 'docling'\n",
    "            }\n",
    "            \n",
    "            paper_info['full_text'] = f\"{paper_info['title']} {paper_info['abstract']} {paper_info['keywords']}\"\n",
    "            \n",
    "            papers.append(paper_info)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Skipped {json_file.name}: {e}\")\n",
    "\n",
    "df = pd.DataFrame(papers)\n",
    "\n",
    "# --- CLEANING LOGIC ---\n",
    "usable_mask = (df['title'].str.len() > 10) & (df['abstract'].str.len() > 30)\n",
    "df_clean = df[usable_mask].copy()\n",
    "\n",
    "# --- SAVE ---\n",
    "df_clean.to_csv(output_dir / \"iccs_docling_cleaned.csv\", index=False)\n",
    "print(f\"\\n[DOCLING] Success! Loaded {len(df)} papers. {len(df_clean)} are high quality.\")\n",
    "\n",
    "fields = ['title', 'keywords', 'abstract']\n",
    "print(\"-\" * 30)\n",
    "print(\"MISSING DATA AUDIT (DOCLING)\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "for field in fields:\n",
    "    is_missing = df[field].isna() | (df[field].astype(str).str.strip() == \"\")\n",
    "    missing_count = is_missing.sum()\n",
    "    missing_pct = (missing_count / len(df)) * 100\n",
    "    print(f\"{field.capitalize():<10}: {missing_count:>4} papers missing ({missing_pct:>5.1f}%)\")\n",
    "\n",
    "all_missing = (\n",
    "    (df['title'].astype(str).str.strip() == \"\") &\n",
    "    (df['keywords'].astype(str).str.strip() == \"\") &\n",
    "    (df['abstract'].astype(str).str.strip() == \"\")\n",
    ").sum()\n",
    "\n",
    "print(f\"Total Completely Empty: {all_missing} papers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bf88e6",
   "metadata": {},
   "source": [
    "Merging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32f7520d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets...\n",
      "Grobid papers: 8118\n",
      "Docling papers: 5904\n",
      "Repairing Titles, Abstracts, and Keywords...\n",
      "\n",
      "MERGE COMPLETE\n",
      "Total Papers in Master Dataset: 8181\n",
      "Repaired Year Range: 2001 - 2025\n",
      "Master file saved to: iccs_analysis\\iccs_master_dataset.csv\n",
      "\n",
      "FINAL MASTER DATASET AUDIT\n",
      "----------------------------------------\n",
      "Title     :    0 papers missing (  0.0%)\n",
      "Keywords  : 2913 papers missing ( 35.6%)\n",
      "Abstract  :    0 papers missing (  0.0%)\n",
      "----------------------------------------\n",
      "Total Completely Empty : 0 papers\n",
      "Total Usable for ML    : 8181 papers (Title + Abstract)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "# We assume the CSVs were generated in the 'iccs_analysis' folder by the previous scripts\n",
    "GROBID_CSV_PATH = Path(\"iccs_analysis/iccs_grobid_cleaned.csv\")\n",
    "DOCLING_CSV_PATH = Path(\"iccs_analysis/iccs_docling_cleaned.csv\")\n",
    "OUTPUT_MASTER_PATH = Path(\"iccs_analysis/iccs_master_dataset.csv\")\n",
    "\n",
    "# 1. Load your two datasets\n",
    "print(\"Loading datasets...\")\n",
    "if not GROBID_CSV_PATH.exists() or not DOCLING_CSV_PATH.exists():\n",
    "    print(f\"ERROR: One or both input files not found in 'iccs_analysis/'.\")\n",
    "    print(f\"Check: {GROBID_CSV_PATH} and {DOCLING_CSV_PATH}\")\n",
    "    exit()\n",
    "\n",
    "df_grobid = pd.read_csv(GROBID_CSV_PATH)\n",
    "df_docling = pd.read_csv(DOCLING_CSV_PATH)\n",
    "\n",
    "# Ensure filenames are strings for consistent merging\n",
    "df_grobid['filename'] = df_grobid['filename'].astype(str)\n",
    "df_docling['filename'] = df_docling['filename'].astype(str)\n",
    "\n",
    "print(f\"Grobid papers: {len(df_grobid)}\")\n",
    "print(f\"Docling papers: {len(df_docling)}\")\n",
    "\n",
    "# 2. Perform an \"Outer Join\" on filename to include every paper found by either parser\n",
    "df_combined = pd.merge(df_grobid, df_docling, on='filename', how='outer', suffixes=('_g', '_d'))\n",
    "\n",
    "# Helper function to pick the \"Better\" (longer/non-null) string\n",
    "def pick_best(val_g, val_d):\n",
    "    # Convert to string and handle NaNs/None\n",
    "    s_g = str(val_g).strip() if pd.notna(val_g) and str(val_g).lower() != 'nan' else \"\"\n",
    "    s_d = str(val_d).strip() if pd.notna(val_d) and str(val_d).lower() != 'nan' else \"\"\n",
    "    \n",
    "    # If one is empty, return the other. If both have text, return the longest one.\n",
    "    if not s_g: return s_d\n",
    "    if not s_d: return s_g\n",
    "    return s_g if len(s_g) >= len(s_d) else s_d\n",
    "\n",
    "# 3. Apply the \"Mutual Healing\" logic to all key fields\n",
    "print(\"Repairing Titles, Abstracts, and Keywords...\")\n",
    "df_combined['final_title'] = df_combined.apply(lambda x: pick_best(x['title_g'], x['title_d']), axis=1)\n",
    "df_combined['final_abstract'] = df_combined.apply(lambda x: pick_best(x['abstract_g'], x['abstract_d']), axis=1)\n",
    "df_combined['final_keywords'] = df_combined.apply(lambda x: pick_best(x['keywords_g'], x['keywords_d']), axis=1)\n",
    "\n",
    "# 4. Repair the Year (Take either, prioritizing Grobid's extraction if valid)\n",
    "# This prioritization works because Grobid covered all years consistently\n",
    "df_combined['final_year'] = df_combined['year_g'].fillna(df_combined['year_d']).replace(0, np.nan)\n",
    "\n",
    "# 5. Final Cleanup\n",
    "df_final = df_combined[[\n",
    "    'filename', \n",
    "    'final_title', \n",
    "    'final_abstract', \n",
    "    'final_keywords', \n",
    "    'final_year'\n",
    "]].copy()\n",
    "\n",
    "df_final.columns = ['filename', 'title', 'abstract', 'keywords', 'year']\n",
    "\n",
    "# 6. Final Filter: Remove rows where both title AND abstract are still missing (the 'completely empty' ones)\n",
    "df_final = df_final.dropna(subset=['title', 'abstract'], how='all')\n",
    "\n",
    "# Output Results\n",
    "print(f\"\\nMERGE COMPLETE\")\n",
    "print(f\"Total Papers in Master Dataset: {len(df_final)}\")\n",
    "if df_final['year'].notna().any():\n",
    "    print(f\"Repaired Year Range: {int(df_final['year'].min())} - {int(df_final['year'].max())}\")\n",
    "\n",
    "# Save the final masterpiece\n",
    "df_final.to_csv(OUTPUT_MASTER_PATH, index=False)\n",
    "print(f\"Master file saved to: {OUTPUT_MASTER_PATH}\")\n",
    "\n",
    "# --- AUDIT SECTION ---\n",
    "df_audit = df_final \n",
    "\n",
    "# Define the fields to check\n",
    "fields = ['title', 'keywords', 'abstract']\n",
    "\n",
    "print(\"\\nFINAL MASTER DATASET AUDIT\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for field in fields:\n",
    "    # Check for both actual NaNs and empty strings/whitespace\n",
    "    # We use .fillna('') first to ensure the string strip doesn't fail on NaNs\n",
    "    is_missing = df_audit[field].fillna('').astype(str).str.strip() == \"\"\n",
    "    missing_count = is_missing.sum()\n",
    "    missing_pct = (missing_count / len(df_audit)) * 100\n",
    "    \n",
    "    print(f\"{field.capitalize():<10}: {missing_count:>4} papers missing ({missing_pct:>5.1f}%)\")\n",
    "\n",
    "# 1. Check for \"Completely Empty\" (missing all three metadata fields)\n",
    "all_missing_mask = (\n",
    "    (df_audit['title'].fillna('').str.strip() == \"\") &\n",
    "    (df_audit['keywords'].fillna('').str.strip() == \"\") &\n",
    "    (df_audit['abstract'].fillna('').str.strip() == \"\")\n",
    ")\n",
    "all_missing_count = all_missing_mask.sum()\n",
    "\n",
    "# 2. Check for \"Usable\" (Has Title AND Abstract)\n",
    "# This is your key metric for Big Data / ML School\n",
    "usable_mask = (\n",
    "    (df_audit['title'].fillna('').str.strip() != \"\") & \n",
    "    (df_audit['abstract'].fillna('').str.strip().str.len() > 50)\n",
    ")\n",
    "usable_count = usable_mask.sum()\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Completely Empty : {all_missing_count} papers\")\n",
    "print(f\"Total Usable for ML    : {usable_count} papers (Title + Abstract)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86cb4c8",
   "metadata": {},
   "source": [
    "Audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4976c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Master Dataset: 8182 records\n",
      "\n",
      "--- TEST 1: TITLE REPAIRS ---\n",
      "No title repairs found (Grobid titles were already good or dropped).\n",
      "------------------------------\n",
      "--- TEST 2: ABSTRACT REPAIRS ---\n",
      "File: 2022_10.1007_978-3-031-08757-8_50.json\n",
      "BEFORE (Grobid): Graphics-Visualization-Computing Lab (GVCL),\n",
      "AFTER  (Master): Automated interpretation of digital images of charts in documents and the internet helps to improve the accessibility of visual representation of data. One of the approaches for automation involves ex...\n",
      "Repaired 1 abstracts using Docling.\n",
      "------------------------------\n",
      "--- TEST 3: LENGTH COMPARISON (Docling vs Grobid) ---\n",
      "File: 2001_10.1007_3-540-45545-0_1.j... | Grobid: 1173 chars | Master: 1173 chars -> SAME\n",
      "File: 2001_10.1007_3-540-45545-0_10.... | Grobid: 459 chars | Master: 459 chars -> SAME\n",
      "File: 2001_10.1007_3-540-45545-0_101... | Grobid: 498 chars | Master: 498 chars -> SAME\n",
      "File: 2001_10.1007_3-540-45545-0_102... | Grobid: 547 chars | Master: 547 chars -> SAME\n",
      "File: 2001_10.1007_3-540-45545-0_103... | Grobid: 258 chars | Master: 258 chars -> SAME\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Load your files\n",
    "grobid_path = Path(\"iccs_analysis/iccs_grobid_cleaned.csv\")\n",
    "docling_path = Path(\"iccs_analysis/iccs_docling_cleaned.csv\")\n",
    "master_path = Path(\"iccs_analysis/iccs_master_dataset.csv\")\n",
    "\n",
    "df_g = pd.read_csv(grobid_path).set_index(\"filename\")\n",
    "df_d = pd.read_csv(docling_path).set_index(\"filename\")\n",
    "df_m = pd.read_csv(master_path).set_index(\"filename\")\n",
    "\n",
    "print(f\"Loaded Master Dataset: {len(df_m)} records\\n\")\n",
    "\n",
    "# --- TEST 1: Did we actually fix broken Titles? ---\n",
    "# Find papers where Grobid title was missing/short, but Master has it.\n",
    "print(\"--- TEST 1: TITLE REPAIRS ---\")\n",
    "bad_grobid_title = df_g[df_g['title'].str.len() < 10].index\n",
    "repaired_titles = df_m.loc[df_m.index.intersection(bad_grobid_title)]\n",
    "\n",
    "if len(repaired_titles) > 0:\n",
    "    sample = repaired_titles.iloc[0]\n",
    "    fname = sample.name\n",
    "    print(f\"File: {fname}\")\n",
    "    print(f\"BEFORE (Grobid): {df_g.loc[fname, 'title']}\")\n",
    "    print(f\"AFTER  (Master): {sample['title']}\")\n",
    "    print(f\"Repaired {len(repaired_titles)} titles using Docling.\")\n",
    "else:\n",
    "    print(\"No title repairs found (Grobid titles were already good or dropped).\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- TEST 2: Did we actually fix broken Abstracts? ---\n",
    "# Find papers where Grobid abstract was missing/short, but Master has a long one.\n",
    "print(\"--- TEST 2: ABSTRACT REPAIRS ---\")\n",
    "bad_grobid_abs = df_g[df_g['abstract'].str.len() < 50].index\n",
    "repaired_abs = df_m.loc[df_m.index.intersection(bad_grobid_abs)]\n",
    "\n",
    "# Filter for ones that are actually good now\n",
    "repaired_abs = repaired_abs[repaired_abs['abstract'].str.len() > 100]\n",
    "\n",
    "if len(repaired_abs) > 0:\n",
    "    sample = repaired_abs.iloc[0]\n",
    "    fname = sample.name\n",
    "    print(f\"File: {fname}\")\n",
    "    print(f\"BEFORE (Grobid): {df_g.loc[fname, 'abstract']}\")\n",
    "    print(f\"AFTER  (Master): {sample['abstract'][:200]}...\") # truncate for display\n",
    "    print(f\"Repaired {len(repaired_abs)} abstracts using Docling.\")\n",
    "else:\n",
    "    print(\"No abstract repairs found.\")\n",
    "\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# --- TEST 3: Did we get the longer text? ---\n",
    "# Check 5 random common files to see if text length improved\n",
    "print(\"--- TEST 3: LENGTH COMPARISON (Docling vs Grobid) ---\")\n",
    "common_files = df_g.index.intersection(df_d.index)\n",
    "if len(common_files) > 0:\n",
    "    sample_files = common_files[:5]\n",
    "    for fname in sample_files:\n",
    "        len_g = len(str(df_g.loc[fname, 'abstract']))\n",
    "        len_m = len(str(df_m.loc[fname, 'abstract']))\n",
    "        \n",
    "        status = \"SAME\"\n",
    "        if len_m > len_g: status = \"IMPROVED\"\n",
    "        \n",
    "        print(f\"File: {fname[:30]}... | Grobid: {len_g} chars | Master: {len_m} chars -> {status}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bbbc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
