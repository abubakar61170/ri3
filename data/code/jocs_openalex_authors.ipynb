{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e988e52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "\n",
    "INPUT_FILE = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\gold_jocs_clean.jsonl\"\n",
    "OUTPUT_FILE = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\author_profiles.jsonl\"\n",
    "EMAIL = \"nehalsonu4@gmial.com\" \n",
    "\n",
    "def get_unique_author_ids(path):\n",
    "    authors = set()\n",
    "    print(\"Unique Author IDs nikaal raha hoon...\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                authorships = record.get('openalex_work', {}).get('authorships', [])\n",
    "                for auth in authorships:\n",
    "                    a_id = auth.get('author', {}).get('id')\n",
    "                    if a_id:\n",
    "                        # Extracting ID (e.g., A5022139811)\n",
    "                        authors.add(a_id.split('/')[-1])\n",
    "            except Exception:\n",
    "                continue\n",
    "    return list(authors)\n",
    "\n",
    "def fetch_author_object(author_id):\n",
    "    \"\"\"Sirf Author ka Profile Object fetch karne ke liye\"\"\"\n",
    "    url = f\"https://api.openalex.org/authors/{author_id}?mailto={EMAIL}\"\n",
    "    try:\n",
    "        r = requests.get(url, timeout=15)\n",
    "        if r.status_code == 200:\n",
    "            return r.json()\n",
    "        elif r.status_code == 429: # Rate limit handle karne ke liye\n",
    "            time.sleep(2)\n",
    "            return fetch_author_object(author_id)\n",
    "    except Exception:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "# Unique IDs ki list\n",
    "unique_ids = get_unique_author_ids(INPUT_FILE)\n",
    "print(f\"Total {len(unique_ids)} unique authors miley hain.\")\n",
    "\n",
    "# Parallel Fetching (Phase 1: Profiles Only)\n",
    "print(\"OpenAlex se Author Objects fetch ho rahay hain...\")\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n",
    "    # 10 threads kaafi hain polite pool ke liye\n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for result in tqdm(executor.map(fetch_author_object, unique_ids), total=len(unique_ids)):\n",
    "            if result:\n",
    "                f_out.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "print(f\"Done! Saaray Author Objects yahan save hain: {OUTPUT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae14583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "EMAIL = \"nehsalsonu4@gmail.com\"\n",
    "INPUT_FILE = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\gold_jocs_clean.jsonl\"\n",
    "OUTPUT_DIR = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\"\n",
    "# OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"authors_work.jsonl\")\n",
    "OUTPUT_FILE = os.path.join(OUTPUT_DIR, \"authors_work.parquet\")\n",
    "\n",
    "# Ensure directory exists\n",
    "if not os.path.exists(OUTPUT_DIR):\n",
    "    os.makedirs(OUTPUT_DIR)\n",
    "\n",
    "def get_unique_author_ids(path):\n",
    "    authors = set()\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                record = json.loads(line)\n",
    "                authorships = record.get('openalex_work', {}).get('authorships', [])\n",
    "                for auth in authorships:\n",
    "                    a_id = auth.get('author', {}).get('id')\n",
    "                    if a_id: authors.add(a_id.split('/')[-1])\n",
    "            except: continue\n",
    "    return list(authors)\n",
    "\n",
    "def fetch_full_career(author_id):\n",
    "    \"\"\"Hits the works_api_url and follows cursors until ALL papers are fetched\"\"\"\n",
    "    all_works = []\n",
    "    cursor = '*' # Starting cursor for deep paging\n",
    "    base_url = \"https://api.openalex.org/works\"\n",
    "    \n",
    "    while cursor:\n",
    "        params = {\n",
    "            'filter': f'author.id:{author_id}',\n",
    "            'select': 'id,title,abstract_inverted_index,publication_year,authorships,concepts',\n",
    "            'per_page': 200, # Max allowed for faster fetching\n",
    "            'cursor': cursor,\n",
    "            'mailto': EMAIL\n",
    "        }\n",
    "        try:\n",
    "            r = requests.get(base_url, params=params, timeout=30)\n",
    "            if r.status_code == 200:\n",
    "                data = r.json()\n",
    "                results = data.get('results', [])\n",
    "                all_works.extend(results)\n",
    "                \n",
    "                # Check for next page\n",
    "                next_cursor = data.get('meta', {}).get('next_cursor')\n",
    "                if next_cursor and results:\n",
    "                    cursor = next_cursor\n",
    "                else:\n",
    "                    cursor = None # Stop when no more results\n",
    "            else:\n",
    "                cursor = None \n",
    "        except Exception:\n",
    "            time.sleep(1) # Simple retry delay\n",
    "            continue\n",
    "            \n",
    "    return {\"author_id\": author_id, \"total_works\": len(all_works), \"works\": all_works}\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# unique_ids = get_unique_author_ids(INPUT_FILE)\n",
    "# print(f\"Found {len(unique_ids)} authors. Fetching EVERY single paper in their careers...\")\n",
    "\n",
    "# with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n",
    "#     # 8 workers are optimal for OpenAlex Polite Pool\n",
    "#     with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "#         for result in tqdm(executor.map(fetch_full_career, unique_ids), total=len(unique_ids)):\n",
    "#             if result and result['works']:\n",
    "#                 f_out.write(json.dumps(result) + \"\\n\")\n",
    "\n",
    "# print(f\"MISSION COMPLETE! Full career data saved to: {OUTPUT_FILE}\")\n",
    "\n",
    "# --- Naya Execution Block ---\n",
    " # Result jama karne ke liye list\n",
    "\n",
    "unique_ids = get_unique_author_ids(INPUT_FILE)\n",
    "print(f\"Found {len(unique_ids)} authors. Fetching EVERY single paper in their careers...\")\n",
    "    \n",
    "all_results = []\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=8) as executor:\n",
    "    # Saare results ko ek list mein collect karein\n",
    "    for result in tqdm(executor.map(fetch_full_career, unique_ids), total=len(unique_ids)):\n",
    "        if result and result['works']:\n",
    "            all_results.append(result)\n",
    "\n",
    "print(\"Finalizing Parquet file...\")\n",
    "# List ko DataFrame mein badlein\n",
    "df = pd.DataFrame(all_results)\n",
    "\n",
    "# Parquet format mein save karein (Binary format)\n",
    "df.to_parquet(OUTPUT_FILE, engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35ca09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Aapki file ka sahi path\n",
    "input_file = r\"D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\processed\\authors_work.jsonl\"\n",
    "\n",
    "total_papers = 0\n",
    "total_authors = 0\n",
    "papers_with_abstract = 0\n",
    "\n",
    "print(\"Auditing Works and Abstracts...\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in tqdm(f):\n",
    "        try:\n",
    "            data = json.loads(line)\n",
    "            total_authors += 1\n",
    "            \n",
    "            # Author ke total works ka count (meta data se)\n",
    "            total_papers += data.get('total_works', 0)\n",
    "            \n",
    "            # Har individual paper ko check karna\n",
    "            works_list = data.get('works', [])\n",
    "            for work in works_list:\n",
    "                # Check if abstract_inverted_index exists and is not empty\n",
    "                if work.get('abstract_inverted_index'):\n",
    "                    papers_with_abstract += 1\n",
    "                    \n",
    "        except Exception as e:\n",
    "            continue\n",
    "\n",
    "# Coverage Calculation\n",
    "if total_papers > 0:\n",
    "    coverage = (papers_with_abstract / total_papers) * 100\n",
    "else:\n",
    "    coverage = 0\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Total Authors Processed: {total_authors}\")\n",
    "print(f\"Total Papers Found:      {total_papers}\")\n",
    "print(f\"Papers with Abstracts:   {papers_with_abstract}\")\n",
    "print(f\"Abstract Coverage:       {coverage:.2f}%\")\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
