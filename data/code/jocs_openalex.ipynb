{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0189481b",
   "metadata": {},
   "source": [
    "# JOCS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48e0c40",
   "metadata": {},
   "source": [
    "## OpenAlex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98d55c7",
   "metadata": {},
   "source": [
    "using DBLP metadata, targeting DOI and Title at fallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d6d228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Querying 2106 DOIs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/43 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request error: HTTPSConnectionPool(host='api.openalex.org', port=443): Read timed out.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 43/43 [02:08<00:00,  2.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Falling back to title search for 75 items...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75/75 [01:24<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished. Saved 2092 records.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import time\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from requests.adapters import HTTPAdapter\n",
    "from urllib3.util.retry import Retry\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "\n",
    "DBLP_DIR = Path(os.environ[\"DBLP_DIR\"])\n",
    "OPENALEX_DIR = Path(os.environ[\"OPENALEX_DIR\"])\n",
    "IN_PATH = DBLP_DIR/\"interim\"/\"jocs\"/\"jocs_dblp_dois.jsonl\"\n",
    "OUT_RAW_JSONL = OPENALEX_DIR/\"raw\"/\"jocs\"/\"JoCS_openalex_works.jsonl\"\n",
    "# OUT_CORE_PARQUET = OPENALEX_DIR/\"raw\"/\"jocs\"/\"JoCS_openalex_works.parquet\"\n",
    "\n",
    "OPENALEX_BASE = os.getenv(\"OA_BASE\")\n",
    "USER_AGENT = os.getenv(\"USER_OA\")\n",
    "\n",
    "BATCH_SIZE = 50\n",
    "SLEEP_SEC = 1.0\n",
    "\n",
    "session = requests.Session()\n",
    "retries = Retry(total=5, backoff_factor=1, status_forcelist=[429, 500, 502, 503, 504])\n",
    "session.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "\n",
    "def normalize_doi(doi):\n",
    "    if not doi:\n",
    "        return None\n",
    "    doi = str(doi).strip().lower()\n",
    "    for prefix in (\"https://doi.org/\", \"http://doi.org/\", \"doi:\"):\n",
    "        if doi.startswith(prefix):\n",
    "            doi = doi[len(prefix):]\n",
    "    return doi\n",
    "\n",
    "def openalex_get(url, params=None):\n",
    "    headers = {\"User-Agent\": USER_AGENT}\n",
    "    try:\n",
    "        r = session.get(url, headers=headers, params=params, timeout=60)\n",
    "        if r.status_code == 404:\n",
    "            return None\n",
    "        r.raise_for_status()\n",
    "        return r.json()\n",
    "    except Exception as e:\n",
    "        print(f\"Request error: {e}\")\n",
    "        return None\n",
    "\n",
    "rows = []\n",
    "with open(IN_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip():\n",
    "            continue\n",
    "        data = json.loads(line)\n",
    "        rows.append({\n",
    "            \"dblp_id\": data.get(\"key\"),\n",
    "            \"doi\": data.get(\"doi\"),\n",
    "            \"title\": data.get(\"title\"),\n",
    "            \"year\": data.get(\"year\"),\n",
    "            \"volume\": data.get(\"volume\")\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "df[\"year_int\"] = pd.to_numeric(df[\"year\"], errors='coerce')\n",
    "df = df[df['year_int'].between(2010, 2026)].copy()\n",
    "df[\"normalized_doi\"] = df[\"doi\"].apply(normalize_doi)\n",
    "\n",
    "doi_to_indices = {}\n",
    "for idx, doi in df[\"normalized_doi\"].items():\n",
    "    if doi:\n",
    "        doi_to_indices.setdefault(doi, []).append(idx)\n",
    "\n",
    "all_dois = list(doi_to_indices.keys())\n",
    "work_by_doi = {}\n",
    "\n",
    "print(f\"Querying {len(all_dois)} DOIs...\")\n",
    "for i in tqdm(range(0, len(all_dois), BATCH_SIZE)):\n",
    "    batch = all_dois[i:i+BATCH_SIZE]\n",
    "    filt = \"doi:\" + \"|\".join(batch)\n",
    "    params = {\"filter\": filt, \"per-page\": 200}\n",
    "    \n",
    "    data = openalex_get(f\"{OPENALEX_BASE}/works\", params=params)\n",
    "    if data:\n",
    "        for w in data.get(\"results\", []):\n",
    "            nd = normalize_doi(w.get(\"doi\"))\n",
    "            if nd:\n",
    "                work_by_doi[nd] = w\n",
    "    time.sleep(SLEEP_SEC)\n",
    "    \n",
    "# Fallback\n",
    "def search_by_title(title, year):\n",
    "    if not title:\n",
    "        return None\n",
    "    params = {\"search\": title.strip(), \"per-page\": 5}\n",
    "    if year and not pd.isna(year):\n",
    "        params[\"filter\"] = f\"from_publication_date:{int(year)}-01-01,to_publication_date:{int(year)}-12-31\"\n",
    "    \n",
    "    res = openalex_get(f\"{OPENALEX_BASE}/works\", params=params)\n",
    "    if res and res.get(\"results\"):\n",
    "        return res.get(\"results\")[0]\n",
    "    return None\n",
    "\n",
    "missing = list(set(all_dois) - set(work_by_doi.keys()))\n",
    "print(f\"Falling back to title search for {len(missing)} items...\")\n",
    "for doi in tqdm(missing):\n",
    "    idx = doi_to_indices[doi][0]\n",
    "    row = df.loc[idx]\n",
    "    w = search_by_title(row[\"title\"], row[\"year_int\"])\n",
    "    if w:\n",
    "        work_by_doi[doi] = w\n",
    "    time.sleep(0.5)\n",
    "\n",
    "OUT_RAW_JSONL.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUT_RAW_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for _, row in df.iterrows():\n",
    "        nd = row[\"normalized_doi\"]\n",
    "        item = {\n",
    "            \"dblp_id\": row[\"dblp_id\"],\n",
    "            \"dblp_title\": row[\"title\"],\n",
    "            \"dblp_year\": row[\"year\"],\n",
    "            \"dblp_doi\": row[\"doi\"],\n",
    "            \"normalized_doi\": nd,\n",
    "            \"openalex_work\": work_by_doi.get(nd),\n",
    "        }\n",
    "        f.write(json.dumps(item, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "core_rows = []\n",
    "with open(OUT_RAW_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        rec = json.loads(line)\n",
    "        w = rec.get(\"openalex_work\")\n",
    "        if not w:\n",
    "            continue\n",
    "\n",
    "        loc = w.get(\"primary_location\") or {}\n",
    "        src = loc.get(\"source\") or {}\n",
    "\n",
    "        core_rows.append({\n",
    "            \"dblp_id\": rec[\"dblp_id\"],\n",
    "            \"dblp_title\": rec[\"dblp_title\"],\n",
    "            \"dblp_year\": rec[\"dblp_year\"],\n",
    "            \"work_id\": w.get(\"id\"),\n",
    "            \"display_name\": w.get(\"display_name\"),\n",
    "            \"cited_by_count\": w.get(\"cited_by_count\"),\n",
    "            \"is_oa\": (w.get(\"open_access\") or {}).get(\"is_oa\"),\n",
    "            \"source_name\": src.get(\"display_name\"),\n",
    "            \"topics\": json.dumps(w.get(\"topics\", []), ensure_ascii=False)\n",
    "        })\n",
    "\n",
    "if core_rows:\n",
    "    # pd.DataFrame(core_rows).to_parquet(OUT_CORE_PARQUET, index=False)\n",
    "    print(f\"Finished. Saved {len(core_rows)} records.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f732de98",
   "metadata": {},
   "source": [
    "## Filling missing author's Affiliation Data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68c19b3",
   "metadata": {},
   "source": [
    "using GROBID Parsed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2791b605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GROBID JSON files...\n",
      "Loaded 2095 DOIs from GROBID.\n",
      "Merging data...\n",
      "DOI Matches found: 2091/2106\n",
      "Gaps filled in 'raw_pdf_affiliation': 710\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "OPENALEX_DIR = Path(os.environ[\"OPENALEX_DIR\"])\n",
    "\n",
    "OPENALEX_JSONL = OPENALEX_DIR /\"raw\"/\"jocs\"/\"JoCS_openalex_works.jsonl\"\n",
    "GROBID_DIR = Path(os.getenv(\"JOCS_PARSED_GROBID\"))\n",
    "OUTPUT_FILE = Path(os.environ[\"JOCS_ENRICHED_OA_GROBID_DIR\"]) / \"JoCS_openalex_enriched.jsonl\"\n",
    "\n",
    "def normalize_name(name):\n",
    "    if not name: return \"\"\n",
    "    name = name.replace(\".\", \" \").lower()\n",
    "    return re.sub(r'\\s+', ' ', name).strip()\n",
    "\n",
    "def extract_doi_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Filename: '2023_10.1016_j.jocs.2023.102166.json'\n",
    "    DOI: '10.1016/j.jocs.2023.102166'\n",
    "    \"\"\"\n",
    "    # Regex for DOI pattern\n",
    "    match = re.search(r'10\\.\\d{4,}_.*(?=\\.json)', filename)\n",
    "    if match:\n",
    "        doi_part = match.group(0)\n",
    "        # Pehle underscore ko slash mein badlein\n",
    "        return doi_part.replace(\"_\", \"/\", 1).lower().strip()\n",
    "    return None\n",
    "\n",
    "# Load GROBID data\n",
    "grobid_lookup = {}\n",
    "print(\"Loading GROBID JSON files...\")\n",
    "for fpath in list(GROBID_DIR.glob(\"*.json\")):\n",
    "    doi = extract_doi_from_filename(fpath.name)\n",
    "    if doi:\n",
    "        with open(fpath, 'r', encoding='utf-8') as f:\n",
    "            grobid_lookup[doi] = json.load(f)\n",
    "\n",
    "print(f\"Loaded {len(grobid_lookup)} DOIs from GROBID.\")\n",
    "\n",
    "# Process OpenAlex\n",
    "merged_results = []\n",
    "authors_filled = 0\n",
    "doi_matches = 0\n",
    "total_papers = 0 \n",
    "\n",
    "print(\"Merging data...\")\n",
    "with open(OPENALEX_JSONL, 'r', encoding='utf-8') as f_in:\n",
    "    for line in f_in:\n",
    "        record = json.loads(line)\n",
    "        total_papers += 1\n",
    "        doi = record.get(\"normalized_doi\", \"\").lower().strip()\n",
    "        oa_work = record.get(\"openalex_work\")\n",
    "\n",
    "        # DOI Match Check\n",
    "        if oa_work and doi in grobid_lookup:\n",
    "            doi_matches += 1\n",
    "            grobid_data = grobid_lookup[doi]\n",
    "            authorships = oa_work.get(\"authorships\", [])\n",
    "\n",
    "            for auth_obj in authorships:\n",
    "                # Initialize for matching papers\n",
    "                auth_obj[\"raw_pdf_affiliation\"] = None\n",
    "\n",
    "                # Gap check\n",
    "                has_oa = any([\n",
    "                    len(auth_obj.get(\"institutions\", [])) > 0,\n",
    "                    len(auth_obj.get(\"raw_affiliation_strings\", [])) > 0,\n",
    "                    len(auth_obj.get(\"affiliations\", [])) > 0\n",
    "                ])\n",
    "\n",
    "                if has_oa: continue \n",
    "\n",
    "                # Name Matching\n",
    "                oa_disp = normalize_name(auth_obj.get(\"author\", {}).get(\"display_name\", \"\"))\n",
    "                oa_raw = normalize_name(auth_obj.get(\"raw_author_name\", \"\"))\n",
    "                \n",
    "                for g_auth in grobid_data.get(\"authors\", []):\n",
    "                    g_name = normalize_name(g_auth.get(\"name\", \"\"))\n",
    "                    if (oa_disp == g_name or oa_raw == g_name or oa_disp in g_name or g_name in oa_disp):\n",
    "                        affils = g_auth.get(\"affiliations\", [])\n",
    "                        auth_obj[\"raw_pdf_affiliation\"] = \" ; \".join(affils) if affils else \"Empty in PDF\"\n",
    "                        authors_filled += 1\n",
    "                        break\n",
    "        \n",
    "        merged_results.append(record)\n",
    "\n",
    "OUTPUT_FILE.parent.mkdir(parents=True, exist_ok=True)\n",
    "with open(OUTPUT_FILE, 'w', encoding='utf-8') as f_out:\n",
    "    for res in merged_results:\n",
    "        f_out.write(json.dumps(res, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"DOI Matches found: {doi_matches}/{total_papers}\")\n",
    "print(f\"Gaps filled in 'raw_pdf_affiliation': {authors_filled}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2df73dcd",
   "metadata": {},
   "source": [
    "## Audit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487466ff",
   "metadata": {},
   "source": [
    "CSV of missing authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1380f37f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTHOR AFFILIATION AUDIT\n",
      "Total authors seen:          7,716\n",
      "Authors with NO affiliation: 43\n",
      "\n",
      "Exported 16 papers to D:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\interim\\jocs\\jocs_all_years_missing_authors.csv\n",
      "Total missing authors: 43\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "RAW_PATH = Path(os.getenv(\"JOCS_ENRICHED_OA_GROBID\"))\n",
    "OUTPUT_CSV = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\ri3_repo\\data\\data_files\\openalex\\interim\\jocs\\jocs_all_years_missing_authors.csv\")\n",
    "\n",
    "missing_data = []\n",
    "\n",
    "if not RAW_PATH.exists():\n",
    "    print(f\"Error: {RAW_PATH} not found.\")\n",
    "else:\n",
    "    total_authors = 0\n",
    "    missing_authors = 0\n",
    "    papers_with_missing = 0\n",
    "\n",
    "    with open(RAW_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            if not line.strip():\n",
    "                continue\n",
    "\n",
    "            rec = json.loads(line)\n",
    "            oa_work = rec.get(\"openalex_work\")\n",
    "            if not oa_work:\n",
    "                continue\n",
    "\n",
    "            authorships = oa_work.get(\"authorships\", [])\n",
    "            missing_authors_list = []\n",
    "            missing_count = 0\n",
    "\n",
    "            for auth in authorships:\n",
    "                total_authors += 1\n",
    "\n",
    "                institutions = auth.get(\"institutions\", []) or []\n",
    "                raw_aff = auth.get(\"raw_affiliation_strings\", []) or []\n",
    "                affils = auth.get(\"affiliations\", []) or []\n",
    "                pdf_aff = auth.get(\"raw_pdf_affiliation\")\n",
    "\n",
    "                bad_pdf_vals = {\n",
    "                    None,\n",
    "                    \"Not found in PDF\",\n",
    "                    \"Affiliation listed but empty in PDF\",\n",
    "                    \"Empty in PDF\",\n",
    "                    0,\n",
    "                }\n",
    "\n",
    "                has_any = (\n",
    "                    len(institutions) > 0\n",
    "                    or len(raw_aff) > 0\n",
    "                    or len(affils) > 0\n",
    "                    or (pdf_aff not in bad_pdf_vals)\n",
    "                )\n",
    "\n",
    "                if not has_any:\n",
    "                    missing_authors += 1\n",
    "                    missing_count += 1\n",
    "                    author_name = (\n",
    "                        auth.get(\"author\", {}).get(\"display_name\") \n",
    "                        or auth.get(\"raw_author_name\", \"Unknown\")\n",
    "                    )\n",
    "                    missing_authors_list.append(author_name)\n",
    "\n",
    "            if missing_count > 0:\n",
    "                papers_with_missing += 1\n",
    "                doi = rec.get(\"normalized_doi\") or rec.get(\"dblp_doi\") or \"NO_DOI\"\n",
    "                title = rec.get(\"dblp_title\", \"No title\")\n",
    "                \n",
    "                missing_data.append({\n",
    "                    \"year\": rec.get(\"dblp_year\"),\n",
    "                    \"doi\": doi,\n",
    "                    \"title\": title,\n",
    "                    \"missing_authors\": \"; \".join(missing_authors_list),\n",
    "                    \"total_authors\": len(authorships),\n",
    "                    \"missing_count\": missing_count\n",
    "                })\n",
    "\n",
    "    print(\"AUTHOR AFFILIATION AUDIT\")\n",
    "    print(f\"Total authors seen:          {total_authors:,}\")\n",
    "    print(f\"Authors with NO affiliation: {missing_authors:,}\")\n",
    "\n",
    "    df = pd.DataFrame(missing_data)\n",
    "    df.to_csv(OUTPUT_CSV, index=False)\n",
    "    print(f\"\\nExported {len(missing_data)} papers to {OUTPUT_CSV}\")\n",
    "    print(f\"Total missing authors: {df['missing_count'].sum()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884faa08",
   "metadata": {},
   "source": [
    "#### Removing 2026 papers as they do not have any openalex data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020c1838",
   "metadata": {},
   "source": [
    "Final File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23543055",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JOCS Final Dataset: 2077\n",
      "Removed: 15 proceedings, 14 nulls\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import json\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "CSV_FILE = Path(r\"D:\\ITMO Big Data & ML School\\semester 3\\ri3_repo\\data\\data_files\\openalex\\interim\\jocs\\jocs_all_years_missing_authors.csv\")\n",
    "\n",
    "ENRICHED_JSONL = Path(os.getenv(\"JOCS_ENRICHED_OA_GROBID\"))\n",
    "FINAL_JSONL = Path(os.getenv(\"JOCS_FINAL\"))\n",
    "\n",
    "EXCLUDE_DOIS = {\"10.1016/j.jocs.2024.102462\"}\n",
    "\n",
    "df_missing = pd.read_csv(CSV_FILE)\n",
    "bad_dois = set(df_missing[~df_missing[\"doi\"].isin(EXCLUDE_DOIS)][\"doi\"])\n",
    "\n",
    "complete_records = []\n",
    "proceedings_count = 0\n",
    "oa_null_count = 0\n",
    "total_papers = 0\n",
    "\n",
    "with open(ENRICHED_JSONL, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        if not line.strip(): continue\n",
    "            \n",
    "        rec = json.loads(line)\n",
    "        total_papers += 1\n",
    "        doi = rec.get(\"normalized_doi\") or rec.get(\"dblp_doi\")\n",
    "        \n",
    "        # Filter proceedings\n",
    "        if doi in bad_dois and doi not in EXCLUDE_DOIS:\n",
    "            proceedings_count += 1\n",
    "            continue\n",
    "            \n",
    "        # Filter OpenAlex null\n",
    "        if rec.get(\"openalex_work\") is None:\n",
    "            oa_null_count += 1\n",
    "            continue\n",
    "            \n",
    "        complete_records.append(rec)\n",
    "\n",
    "with open(FINAL_JSONL, \"w\", encoding=\"utf-8\") as f:\n",
    "    for rec in complete_records:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"JOCS Final Dataset: {len(complete_records)}\")\n",
    "print(f\"Removed: {proceedings_count} proceedings, {oa_null_count} nulls\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec44c485",
   "metadata": {},
   "source": [
    "Title + Keywords + abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "081fd222",
   "metadata": {},
   "source": [
    "Now checking Grobid Data to check the trash data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73a0ba69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting analysis in: D:\\ITMO Big Data & ML School\\semester 3\\RI3\\parsed\\jocs_grobid\\all\n",
      "\n",
      "\n",
      "METADATA HEALTH REPORT\n",
      "--------------------------------------------------------------\n",
      "Sit.  | Title    | Keywords   | Abstract   | Count    | Status\n",
      "--------------------------------------------------------------\n",
      "1     | [OK]     | [OK]       | [OK]       | 1987     | PERFECT\n",
      "2     | [MISSING] | [OK]       | [OK]       | 5        | FIXABLE (No Title)\n",
      "3     | [OK]     | [MISSING]  | [OK]       | 36       | COMMON (No Keywords)\n",
      "4     | [OK]     | [OK]       | [MISSING]  | 0        | REVIEW (No Abstract)\n",
      "5     | [MISSING] | [MISSING]  | [OK]       | 12       | BAD (Only Abstract)\n",
      "6     | [MISSING] | [OK]       | [MISSING]  | 2        | BAD (Only Keywords)\n",
      "7     | [OK]     | [MISSING]  | [MISSING]  | 0        | BAD (Only Title)\n",
      "8     | [MISSING] | [MISSING]  | [MISSING]  | 53       | TRASH (Empty)\n",
      "--------------------------------------------------------------\n",
      "TOTAL FILES PROCESSED: 2095\n",
      "\n",
      "Successfully exported 53 records to jocs_trash_files_only.csv\n",
      "Full Path: d:\\ITMO Big Data & ML School\\semester 3\\RI3\\notebooks\\data\\jocs_trash_files_only.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import csv\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Initialize environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Configuration Constants\n",
    "PARSED_PATH = Path(os.getenv(\"JOCS_PARSED_GROBID\", \"\"))\n",
    "OUTPUT_CSV = \"jocs_trash_files_only.csv\"\n",
    "\n",
    "def process_metadata_health(folder_path):\n",
    "    \"\"\"\n",
    "    Analyzes JSON metadata completeness and exports empty records to CSV.\n",
    "    \n",
    "    Args:\n",
    "        folder_path (Path): Path to the directory containing parsed JSON files.\n",
    "    \"\"\"\n",
    "    if not folder_path.exists():\n",
    "        print(f\"Error: Directory not found at {folder_path}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Initialize statistics for the 8 identified metadata situations\n",
    "    # Logic: (Has Title, Has Keywords, Has Abstract)\n",
    "    stats = {\n",
    "        (True, True, True): 0,    # Sit 1: Perfect\n",
    "        (False, True, True): 0,   # Sit 2: No Title\n",
    "        (True, False, True): 0,   # Sit 3: No Keywords\n",
    "        (True, True, False): 0,   # Sit 4: No Abstract\n",
    "        (False, False, True): 0,  # Sit 5: Only Abstract\n",
    "        (False, True, False): 0,  # Sit 6: Only Keywords\n",
    "        (True, False, False): 0,  # Sit 7: Only Title\n",
    "        (False, False, False): 0  # Sit 8: Trash\n",
    "    }\n",
    "\n",
    "    csv_records = []\n",
    "    total_processed = 0\n",
    "\n",
    "    print(f\"Starting analysis in: {folder_path}\\n\")\n",
    "\n",
    "    for file_path in folder_path.glob(\"*.json\"):\n",
    "        total_processed += 1\n",
    "        \n",
    "        try:\n",
    "            with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            print(f\"Warning: Could not process {file_path.name} - {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract and validate fields\n",
    "        raw_title = data.get('title', \"\")\n",
    "        clean_title = str(raw_title).replace('\\n', ' ').strip()\n",
    "        \n",
    "        has_title = bool(clean_title)\n",
    "        has_keywords = bool(data.get('keywords'))\n",
    "        has_abstract = bool(str(data.get('abstract') or \"\").strip())\n",
    "\n",
    "        # Update statistical counters\n",
    "        situation_key = (has_title, has_keywords, has_abstract)\n",
    "        if situation_key in stats:\n",
    "            stats[situation_key] += 1\n",
    "\n",
    "        # Collect data for Situation 8 CSV export\n",
    "        if not has_title and not has_keywords and not has_abstract:\n",
    "            csv_records.append({\n",
    "                \"filename\": file_path.name,\n",
    "                \"extracted_title\": clean_title if clean_title else \"EMPTY / NOT PARSED\",\n",
    "                \"status\": \"Situation 8 (Full Empty)\"\n",
    "            })\n",
    "\n",
    "    # Generate Console Report Table\n",
    "    print_health_report(stats, total_processed)\n",
    "\n",
    "    # Export CSV for Situation 8\n",
    "    if csv_records:\n",
    "        export_to_csv(csv_records, OUTPUT_CSV)\n",
    "    else:\n",
    "        print(\"No files matching Situation 8 criteria were found for export.\")\n",
    "\n",
    "def print_health_report(stats, total):\n",
    "    \"\"\"Prints a formatted summary table of metadata health situations.\"\"\"\n",
    "    header = f\"{'Sit.':<5} | {'Title':<8} | {'Keywords':<10} | {'Abstract':<10} | {'Count':<8} | {'Status'}\"\n",
    "    separator = \"-\" * len(header)\n",
    "    \n",
    "    print(\"\\nMETADATA HEALTH REPORT\")\n",
    "    print(separator)\n",
    "    print(header)\n",
    "    print(separator)\n",
    "\n",
    "    rows = [\n",
    "        (1, True, True, True, \"PERFECT\"),\n",
    "        (2, False, True, True, \"FIXABLE (No Title)\"),\n",
    "        (3, True, False, True, \"COMMON (No Keywords)\"),\n",
    "        (4, True, True, False, \"REVIEW (No Abstract)\"),\n",
    "        (5, False, False, True, \"BAD (Only Abstract)\"),\n",
    "        (6, False, True, False, \"BAD (Only Keywords)\"),\n",
    "        (7, True, False, False, \"BAD (Only Title)\"),\n",
    "        (8, False, False, False, \"TRASH (Empty)\")\n",
    "    ]\n",
    "\n",
    "    for sit, t, k, a, status in rows:\n",
    "        t_marker = \"[OK]\" if t else \"[MISSING]\"\n",
    "        k_marker = \"[OK]\" if k else \"[MISSING]\"\n",
    "        a_marker = \"[OK]\" if a else \"[MISSING]\"\n",
    "        count = stats.get((t, k, a), 0)\n",
    "        print(f\"{sit:<5} | {t_marker:<8} | {k_marker:<10} | {a_marker:<10} | {count:<8} | {status}\")\n",
    "\n",
    "    print(separator)\n",
    "    print(f\"TOTAL FILES PROCESSED: {total}\\n\")\n",
    "\n",
    "def export_to_csv(records, filename):\n",
    "    \"\"\"Writes identified empty files to a CSV for cleanup audit.\"\"\"\n",
    "    keys = [\"filename\", \"extracted_title\", \"status\"]\n",
    "    try:\n",
    "        with open(filename, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
    "            writer = csv.DictWriter(f, fieldnames=keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(records)\n",
    "        print(f\"Successfully exported {len(records)} records to {filename}\")\n",
    "        print(f\"Full Path: {os.path.abspath(filename)}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error writing CSV: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_metadata_health(PARSED_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e6cf088",
   "metadata": {},
   "source": [
    "Removinf the 53 Trash files form the openalex final data<br>\n",
    "As these are editorial, preface or an update of a paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d1da012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exclusion set initialized with 53 DOIs.\n",
      "Output File:      final_jocs_openalex.jsonl\n",
      "Records Excluded: 40\n",
      "Records Retained: 2037\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Path Configuration\n",
    "INPUT_CSV = \"jocs_trash_files_only.csv\"\n",
    "INPUT_JSONL = Path(os.getenv(\"JOCS_FINAL\"))\n",
    "OUTPUT_JSONL = INPUT_JSONL.parent / \"final_jocs_openalex.jsonl\"\n",
    "\n",
    "def normalize_doi_from_filename(filename):\n",
    "    \"\"\"\n",
    "    Converts filename format to normalized DOI format.\n",
    "    Example: '2010_10.1016_j.jocs.2010.04.003.json' -> '10.1016/j.jocs.2010.04.003'\n",
    "    \"\"\"\n",
    "    # Remove file extension\n",
    "    name_no_ext = os.path.splitext(filename)[0]\n",
    "    \n",
    "    if '_' in name_no_ext:\n",
    "        # Split at the first underscore to remove the year prefix\n",
    "        _, doi_part = name_no_ext.split('_', 1)\n",
    "        # Replace the first underscore in the remaining string with a forward slash\n",
    "        normalized_doi = doi_part.replace('_', '/', 1)\n",
    "        return normalized_doi\n",
    "    \n",
    "    return name_no_ext\n",
    "\n",
    "def generate_final_dataset():\n",
    "    \"\"\"\n",
    "    Filters the input JSONL file by removing records present in the exclusion CSV.\n",
    "    Writes the resulting clean data to a new JSONL file.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(INPUT_CSV):\n",
    "        print(f\"Error: Required exclusion file '{INPUT_CSV}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    if not INPUT_JSONL.exists():\n",
    "        print(f\"Error: Input dataset '{INPUT_JSONL}' not found.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load exclusion list into a set for O(1) lookup performance\n",
    "    try:\n",
    "        df_trash = pd.read_csv(INPUT_CSV)\n",
    "        exclusion_set = set(df_trash['filename'].apply(normalize_doi_from_filename))\n",
    "        print(f\"Exclusion set initialized with {len(exclusion_set)} DOIs.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading exclusion CSV: {e}\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    records_removed = 0\n",
    "    records_retained = 0\n",
    "\n",
    "    # Stream process the JSONL to handle large data volumes\n",
    "    try:\n",
    "        with open(INPUT_JSONL, 'r', encoding='utf-8') as f_in, \\\n",
    "             open(OUTPUT_JSONL, 'w', encoding='utf-8') as f_out:\n",
    "            \n",
    "            for line_number, line in enumerate(f_in, 1):\n",
    "                try:\n",
    "                    record = json.loads(line)\n",
    "                    doi = record.get('normalized_doi', \"\").strip()\n",
    "                    \n",
    "                    if doi in exclusion_set:\n",
    "                        records_removed += 1\n",
    "                        continue\n",
    "                    \n",
    "                    # Write clean records back to the new file\n",
    "                    f_out.write(json.dumps(record, ensure_ascii=False) + '\\n')\n",
    "                    records_retained += 1\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Warning: Skipping malformed JSON on line {line_number}\")\n",
    "                    continue\n",
    "\n",
    "        print(f\"Output File:      {OUTPUT_JSONL.name}\")\n",
    "        print(f\"Records Excluded: {records_removed}\")\n",
    "        print(f\"Records Retained: {records_retained}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during file processing: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    generate_final_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4632abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning for Title Mismatches...\n",
      "Total Records Scanned: 2037\n",
      "Mismatched Records Found: 29\n",
      "\n",
      "Sample Mismatches (First 5):\n",
      "DOI: 10.1016/J.JOCS.2024.102471\n",
      "  DBLP: POD-Galerkin reduced order model coupled with neural networks to solve flow in porous media.\n",
      "  O.A.: Accelerating phase field simulations through a hybrid adaptive Fourier neural operator with U-net backbone\n",
      "DOI: 10.1016/J.JOCS.2024.102492\n",
      "  DBLP: Analytical and numerical methods for the solution to the rigid punch contact integral equations.\n",
      "  O.A.: Thorough investigation of exact wave solutions in nonlinear thermoelasticity theory under the influence of gravity using advanced analytical methods\n",
      "DOI: 10.1016/J.JOCS.2025.102705\n",
      "  DBLP: Efficient numerical simulation of variable-order fractional diffusion processes with a memory kernel.\n",
      "  O.A.: Influence of moving heat sources on thermoviscoelastic behavior of rotating nanorods: a nonlocal Klein–Gordon perspective with fractional heat conduction\n",
      "DOI: 10.1016/J.JOCS.2024.102496\n",
      "  DBLP: A phenomenological discrete model for cardiac tissue mechanics.\n",
      "  O.A.: Intelligent in-cell electrophysiology: Reconstructing intracellular action potentials using a physics-informed deep learning model trained on nanoelectrode array recordings\n",
      "DOI: 10.1016/J.JOCS.2024.102508\n",
      "  DBLP: A novel approach for overlapping community detection in social networks based on the attraction.\n",
      "  O.A.: A comprehensive review on early detection of Alzheimer's disease using various deep learning techniques\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    # Normalize: lowercase and strip spaces\n",
    "    a = str(a).lower().strip()\n",
    "    b = str(b).lower().strip()\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "input_file = r\"D:\\ITMO Big Data & ML School\\semester 3\\ri3_repo\\data\\data_files\\openalex\\processed\\jocs\\final_jocs_openalex.jsonl\"\n",
    "mismatch_count = 0\n",
    "total_records = 0\n",
    "mismatch_examples = []\n",
    "\n",
    "print(\"Scanning for Title Mismatches...\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        total_records += 1\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # 1. DBLP Title\n",
    "        dblp_t = record.get('dblp_title', '')\n",
    "        \n",
    "        # 2. OpenAlex Title\n",
    "        oa_work = record.get('openalex_work', {})\n",
    "        oa_t = oa_work.get('title', '') if oa_work else \"\"\n",
    "        \n",
    "        # Similarity Score\n",
    "        score = string_similarity(dblp_t, oa_t)\n",
    "        \n",
    "        # Agar similarity 80% se kam hai toh masla hai\n",
    "        if score < 0.8:\n",
    "            mismatch_count += 1\n",
    "            if len(mismatch_examples) < 5: # Top 5 examples for debugging\n",
    "                mismatch_examples.append({\n",
    "                    \"doi\": record.get('dblp_doi'),\n",
    "                    \"dblp\": dblp_t,\n",
    "                    \"openalex\": oa_t,\n",
    "                    \"score\": round(score, 2)\n",
    "                })\n",
    "\n",
    "print(f\"Total Records Scanned: {total_records}\")\n",
    "print(f\"Mismatched Records Found: {mismatch_count}\")\n",
    "\n",
    "if mismatch_examples:\n",
    "    print(\"\\nSample Mismatches (First 5):\")\n",
    "    for ex in mismatch_examples:\n",
    "        print(f\"DOI: {ex['doi']}\")\n",
    "        print(f\"  DBLP: {ex['dblp']}\")\n",
    "        print(f\"  O.A.: {ex['openalex']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4d2e4c",
   "metadata": {},
   "source": [
    "### Deleting this Mismatch, came from OpenALex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e158ce31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Professional Purge...\n",
      "Analysis Complete.\n",
      "Total Scanned:   2037\n",
      "Purged Records:  29\n",
      "Retained Gold:   2008\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "\n",
    "def string_similarity(a, b):\n",
    "    # Standard normalization for comparison\n",
    "    a = str(a).lower().strip()\n",
    "    b = str(b).lower().strip()\n",
    "    if not a or not b: return 0\n",
    "    return SequenceMatcher(None, a, b).ratio()\n",
    "\n",
    "# Paths\n",
    "input_file = r\"D:\\ITMO Big Data & ML School\\semester 3\\ri3_repo\\data\\data_files\\openalex\\processed\\jocs\\final_jocs_openalex.jsonl\"\n",
    "output_file = r\"D:\\ITMO Big Data & ML School\\semester 3\\ri3_repo\\data\\data_files\\openalex\\processed\\jocs\\gold_jocs_clean.jsonl\"\n",
    "\n",
    "total_records = 0\n",
    "retained_records = 0\n",
    "deleted_dois = []\n",
    "\n",
    "print(\"Starting Professional Purge...\")\n",
    "\n",
    "with open(input_file, 'r', encoding='utf-8') as f_in, \\\n",
    "     open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "    \n",
    "    for line in f_in:\n",
    "        total_records += 1\n",
    "        record = json.loads(line)\n",
    "        \n",
    "        # Extract titles\n",
    "        dblp_t = record.get('dblp_title', '')\n",
    "        oa_work = record.get('openalex_work', {})\n",
    "        oa_t = oa_work.get('title', '') if isinstance(oa_work, dict) else \"\"\n",
    "        \n",
    "        # Strict Similarity Check\n",
    "        score = string_similarity(dblp_t, oa_t)\n",
    "        \n",
    "        if score >= 0.8:\n",
    "            f_out.write(json.dumps(record) + \"\\n\")\n",
    "            retained_records += 1\n",
    "        else:\n",
    "            deleted_dois.append(record.get('dblp_doi'))\n",
    "\n",
    "print(f\"Analysis Complete.\")\n",
    "print(f\"Total Scanned:   {total_records}\")\n",
    "print(f\"Purged Records:  {len(deleted_dois)}\")\n",
    "print(f\"Retained Gold:   {retained_records}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fcead3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RI_3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
